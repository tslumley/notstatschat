<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.41" />


<title>Superefficiency - A Hugo website</title>
<meta property="og:title" content="Superefficiency - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/notstatschat_1.png"
         width="75"
         height="100"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/tslumley/notstatschat">GitHub</a></li>
    
    <li><a href="https://twitter.com/tslumley">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">4 min read</span>
    

    <h1 class="article-title">Superefficiency</h1>

    
    <span class="article-date">0001/01/01</span>
    

    <div class="article-content">
      <p>If you have <span class="math inline">\(X_1,\ldots,X_n\)</span> independent from an <span class="math inline">\(N(\mu,1)\)</span> distribution you don’t have to think too hard to work out that <span class="math inline">\(\bar X_n\)</span>, the sample mean, is the right estimator of <span class="math inline">\(\mu\)</span> (unless you have quite detailed prior knowledge). As people who have taken an advanced course in mathematical statistics will know, there is a famous estimator that appears to do better. </p>
<p>Hodges’ estimator is given by <span class="math inline">\(H_n=\bar X_n\)</span> if <span class="math inline">\(|\bar X_n|&gt;n^{-1/4}\)</span>, and <span class="math inline">\(H_n=0\)</span> if <span class="math inline">\(|\bar X_n|\leq n^{-1/4}\)</span>. If <span class="math inline">\(\mu\neq 0\)</span>, <span class="math inline">\(H_n=\bar X_n\)</span> for all large enough <span class="math inline">\(n\)</span>, so <span class="math display">\[\sqrt{n}(H_n-\mu)\stackrel{d}{\to}N(0,1)\]</span> just as for <span class="math inline">\(\bar X_n\)</span>. On the other hand, if <span class="math inline">\(\mu=0\)</span>, <span class="math display">\[\sqrt{n}(H_n-\mu)\stackrel{p}{\to}0.\]</span> <span class="math inline">\(H_n\)</span> is asymptotically better than <span class="math inline">\(\bar X_n\)</span> for <span class="math inline">\(\mu=0\)</span> and asymptotically as good for any other value of <span class="math inline">\(\mu\)</span>. Of course there’s something wrong with it: it sucks for <span class="math inline">\(n^{-1/2}\ll\mu&lt;n^{-1/4}\)</span>. Here’s its mean squared error:</p>
<div class="figure">
<img src="https://36.media.tumblr.com/e69fbc5fff24de7c6220af6a7f961db4/tumblr_inline_no7zhmSJD41s1hdxy_540.png" />

</div>
<p>Even <a href="http://en.wikipedia.org/wiki/Hodges%27_estimator">Wikipedia</a> knows this much. What I recently got around to doing was extending this to an estimator that’s asymptotically superior to <span class="math inline">\(\bar X_n\)</span> on a dense set. This isn’t new – Le Cam did it in his PhD thesis. It may even be the same as Le Cam’s construction (which isn’t online, as far as I can tell). [Actually, Le Cam’s construction is a draft exercise in a draft <a href="http://www.stat.yale.edu/~pollard/Courses/618.fall2010/Handouts/Heuristics.pdf">chapter</a> for David Pollard’s long-awaited ‘Asymptopia’. And it <strong>is</strong> basically my one, so it’s quite likely that as a Pollard fan I got at least the idea from there.]</p>
<p>First, instead of just setting the estimate to zero when it’s close enough to zero, we can set it to the nearest integer when it’s close enough to an integer.  Define <span class="math inline">\(\tilde H_n=i\)</span> if <span class="math inline">\(|\bar X_n-i|&lt;0.5n^{-1/4}\)</span>, with <span class="math inline">\(\tilde H_n=\bar X_n\)</span> otherwise.</p>
<p>If <span class="math inline">\(n\)</span> is large enough, we can shrink to multiples of 1/2. For example, using the same threshold for closeness, if <span class="math inline">\(n&gt;16\)</span> there is at most one multiple of 1/2 within <span class="math inline">\(0.5n^{-1/4}\)</span>. If <span class="math inline">\(n&gt;256\)</span> there is at most one multiple of 1/4 within that range.</p>
<p>Define <span class="math inline">\(H_{n,k}=2^{-k}i\)</span> if <span class="math inline">\(|x-2^{-k}i|&lt; 0.5n^{-1/4}\)</span> and <span class="math inline">\(H_{n,k}=\bar X_n\)</span> otherwise. This is well-defined if <span class="math inline">\(n&gt;2^{4k}\)</span>. For any fixed <span class="math inline">\(k\)</span>, <span class="math inline">\(\tilde H_{n,k}\)</span> satisfies <span class="math display">\[\sqrt{n}(H_n-\mu)\stackrel{p}{\to}0\]</span> if <span class="math inline">\(\mu\)</span> is a multiple of <span class="math inline">\(2^{-k}\)</span> and <span class="math display">\[\sqrt{n}(H_n-\mu)\stackrel{d}{\to}N(0,1)\]</span> otherwise.</p>
<p>The obvious thing to do now is to let <span class="math inline">\(k\)</span> increase slowly with <span class="math inline">\(n\)</span>. This doesn’t work. Consider a value for <span class="math inline">\(\\mu\)</span> whose binary expansion has infinitely many 1s, but with increasingly many zeroes between them. Whatever your rule for <span class="math inline">\(k(n)\)</span> there will be values of this type that are close enough to multiples of <span class="math inline">\(2^{-k(n)}\)</span> to get pulled to the wrong value infinitely often as <span class="math inline">\(n\)</span> increases. <span class="math inline">\(H_{n,k(n)}\)</span> will be asymptotically superior to <span class="math inline">\(\\bar X_n\)</span> on a dense set, but it will be asymptotically inferior on another dense set, violating the rules of the game. </p>
<p>What we can do is pick <span class="math inline">\(k\)</span> at random. The efficiency gain isn’t 100% as it was for fixed <span class="math inline">\(k\)</span>, but it’s still there. </p>
<p>Let <span class="math inline">\(K\)</span> be a random variable with probability mass function <span class="math inline">\(p(k)\)</span>, independent of the <span class="math inline">\(X\)</span>s.  The distribution of <span class="math inline">\(H_{n,K}\)</span> conditional on <span class="math inline">\(K=k\)</span> is the distribution of <span class="math inline">\(H_{n,k}\)</span>. If <span class="math inline">\(p(k)&gt;0\)</span> for all <span class="math inline">\(k\)</span>, the probability  of seeing <span class="math inline">\(K=k\)</span> infinitely often is 1, so we can look the limiting distribution of <span class="math inline">\(\sqrt{n}(H_{n,K}-\mu)\)</span> along subsequences with <span class="math inline">\(K=k\)</span>. This limiting distribution is a point mass at zero  if <span class="math inline">\(2^k\\mu\)</span> is an integer, and <span class="math inline">\(N(0,1)\)</span> otherwise. So, <span class="math display">\[\sqrt{n}(H_{n,K}-\mu)\stackrel{d}{\to}q_k\delta_0+(1-q_k)N(0,1)\]</span> where <span class="math display">\[q_k=\sum_k p_k I(2^k\mu\textrm{ is an integer})\]</span></p>
<p>For a dense set of real numbers, and in particular for all numbers representable in binary floating point, <span class="math inline">\(H_{n,K}\)</span> has greater asymptotic efficiency than the efficient estimator <span class="math inline">\(\bar X_n.\)</span>  The disadvantage of this randomised construction is that working out the finite-sample MSE is just horrible to think about.</p>
<p>The other interesting thing to think about is why the ‘overflow’ heuristic doesn’t work. Why doesn’t superefficiency for all fixed <span class="math inline">\(k\)</span> translate into superefficiency for sufficiently-slowly increasing <span class="math inline">\(k(n)\)</span>? As a heuristic, this sort of thing has been around since the early days of analysis, but it’s more than that: the field of non-standard analysis is basically about making it rigorous. </p>
<p>My guess is that <span class="math inline">\(H_{n,k}\)</span> for infinite <span class="math inline">\(n\)</span> is close to the superefficient distribution on the dense set only for ‘large enough’ infinite <span class="math inline">\(k\)</span>, and close to <span class="math inline">\(N(0,1)\)</span> off the dense set only for ‘small enough’ infinite <span class="math inline">\(k\)</span>. The failure of the heuristic is similar to the failure in Cauchy’s invalid proof that a convergent sequence of continuous functinons has a continuous limit, the proof into which later analysis retconned the concepts of ‘uniform convergence’ and ‘equicontinuity’.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>


---
title: 'Code archaeology: polynomial distributed lags'
author: Thomas Lumley
date: '2022-10-14'
slug: code-archaeology-polynomial-distributed-lags
categories: []
tags: []
---



<p>Back in the early 2000s, when I was working on air pollution epidemiology, I wrote some code to fit polynomial distributed lag models. These are a slightly primitive form of regularisation for when you want several lags of an exposure variable as predictors. Last week I was looking for an R package to fit these models, for a student working on Covid wastewater modelling. I didn’t find an R package, but my code was still on the University of Washington website, <a href="https://faculty.washington.edu/tlumley/pdl.R">here</a>.</p>
<p>One fix was needed due to changes in R. The function <code>contr.poly</code> acquired a new second argument, <code>scores</code>, in version 1.8.0, so a call <code>contr.poly(lag,F)</code> needed editing to <code>contr.poly(lag, contrasts=F)</code>. I’ve moved the code to <a href="https://gist.github.com/tslumley/614b8ae9f0207bc602e9a007665846c2">a github gist</a>. Otherwise, the code still does what it did twenty years ago.</p>
<p>So, what do polynomial distributed lag models do? The idea is that you want to fit, say,
<span class="math display">\[E[Y_t]=\alpha+\beta_0X_t+\beta_1X_{t-1}+\beta_2X_{t-2}+\cdots+\beta_KX_{t-K}+\gamma X^*_t\]</span>
Since the lagged values of <span class="math inline">\(X\)</span> are likely to be highly correlated, the individual estimates <span class="math inline">\(\hat\beta_{t-k}\)</span> are going to look strange. They might oscillate a lot, someone of them might have the ‘wrong’ sign, and so on. You might prefer to regularise them so that the coefficients from adjacent lags are close in value. The polynomial distributed lag approach does this smoothing by fitting <span class="math inline">\(\beta_k\)</span> as a polynomial of degree less than <span class="math inline">\(K+1\)</span> in <span class="math inline">\(k\)</span>. Suppose we choose <span class="math inline">\(4\)</span> as the degree. We can create <span class="math inline">\(4\)</span> new variables:
<span class="math display">\[\begin{align} Z_{0t}=1\times X_t+1\times X_{t-1}+1\times X_{t-2}+\cdots&amp;+1\times X_K\\ Z_{1t}=0\times X_t+1\times X_{t-1}+2\times X_{t-2}+\cdots&amp;+K\times X_K\\Z_{2t}=0\times X_t+1\times X_{t-1}+4\times X_{t-2}+\cdots&amp;+K^2\times X_K\\Z_{3t}=0\times X_t+1\times X_{t-1}+8\times X_{t-2}+\cdots&amp;+K^3\times X_K\end{align}.\]</span>
The coefficients <span class="math inline">\(\delta_k\)</span> of these variables are a basis for all the <span class="math inline">\(\beta_k\)</span> that can be written as cubic polynomials in <span class="math inline">\(k\)</span>. So we can fit <code>Y~Z0+Z1+Z2+Z2</code> to get <span class="math inline">\(\hat\delta\)</span>s and take linear combinations of them to get <span class="math inline">\(\hat\beta\)</span>s. We don’t actually use <em>these</em> basis functions, because you get extra numerical analysis coolness points for using an orthogonal basis (hence <code>contr.poly</code>, above), but that’s the idea.</p>
<p>Could we do this with some more modern-looking approach, perhaps using cross-validation to select the smoothness? It’s not easy to use cross-validation, because the goal of regularisation here isn’t to lower the prediction error but to match our prior expectation of smoothness. It would be possible to penalise the differences <span class="math inline">\(\hat\beta_k-\hat\beta_{k-1}\)</span>, much as we do with splines, but the penalty would still have to be chosen based on prior knowledge and/or desired smoothness.</p>
<p>The other thing to mention here is the code. The linear algebra to transform between (<span class="math inline">\(\beta\)</span>, <span class="math inline">\(X\)</span>) and <span class="math inline">\((\delta, Z)\)</span> is not too complicated. Handling the change in the number of columns of the design matrix is trickier. The code uses the <code>specials</code> argument to <code>terms.formula</code>, and Terry Therneau’s <code>untangle.specials</code> function from the <code>survival</code> package, and code I wrote to transform the <code>assign</code> attribute of a model matrix from the format used in R to the format <code>untangle.specials</code> was expecting. A call like</p>
<pre><code>m&lt;-pdlglm(yy~pdl(zz,4,2)+sin(zz))</code></pre>
<p>asking for a 2-df polynomial for four lags of <code>zz</code> calls <code>glm</code> with two linear combinations of the four lags and then rewrites the returned object to include the four coefficients <span class="math inline">\(\beta\)</span> for the four lags (and does similar surgery on the estimated covariance matrix). This would be trivial if there was one <code>pdl</code> term and no other covariates, but the bookkeeping gets a bit annoying when there might be multiple terms and multiple other covariates.</p>

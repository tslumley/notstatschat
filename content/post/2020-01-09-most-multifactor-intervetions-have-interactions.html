---
title: Multifactor interventions and interactions
author: ''
date: '2020-01-09'
slug: most-multifactor-intervetions-have-interactions
categories: []
tags: []
---



<p>The <a href="https://www.methodology.psu.edu/ra/most/">Multiphase Optimisation Strategy</a> for designing multifactor behavioural interventions should be used more. The idea is that you have a lot of potentially good ideas for things that might work, alone or in combination. You don’t want to test them one at a time, because that takes forever. You don’t want to test all against none, because they might not all be compatible, and in any case you don’t want to be stuck doing them all if you don’t need to.</p>
<p>Instead, as any sensible engineering or agricultural researcher would, you put together a small factorial (or fractional factorial) experiment and test them all at once. If you have, say, six factors, you can identify main effects and two-way interactions in a <span class="math inline">\(2^{6-2}\)</span> design, which has only 16 intervention groups. Win!</p>
<p>People who are used to ordinary clinical trials (where a <span class="math inline">\(2^3\)</span> or even <span class="math inline">\(2\times 3\)</span> design is seen as … adventurous) often have unfounded concerns about these designs. There’s a nice <a href="https://www.methodology.psu.edu/ra/most/femiscon/">misconceptions</a> page at the PennState Methodology Center.</p>
<p>However, there’s one ‘misconception’ rebuttal that I’ve seen escape and mutate into a statistical myth in the other direction. And because it’s listed by a highly reputable website as correcting a common misconception, it can be hard to get people to listen</p>
<blockquote>
<p><strong>Misconception 5: There is always less statistical power for interactions than for main effects in a factorial ANOVA. Power decreases as the order of the interaction increases.</strong></p>
</blockquote>
<blockquote>
<p><em>Reality: When effect coding is used, statistical power is the same for all regression coefficients of the same size, whether they correspond to main effects or interactions, and irrespective of the order of the interaction. (Note that the regression coefficient is not the only way to express the effect size of an interaction.)</em></p>
</blockquote>
<p>That is, of course, true. If all your contrast vectors are orthonormal (or orthogonal of any other fixed length), every contrast coefficient is independent and has the same variance, so the power is the same. Hooray for balanced designs.</p>
<p>The note is also important, and that’s what people sometimes miss. I have seen, for example, a claim that if you use effect coding rather than dummy variable coding, the power for detecting an interaction increases. It doesn’t, because the models with and without the interaction are the same two models under either coding, just reparametrised. The test is the same; it must have the same power.</p>
<p>Suppose we have the world’s least interesting interaction test. We have a <span class="math inline">\(2\times 2\)</span> factorial design of treatments A and B, with the following means for <span class="math inline">\(Y\)</span> in a linear model</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>not A</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>B</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="even">
<td>not B</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Clearly, there is an interaction. We could define variables <span class="math inline">\(x_A, x_B, x_{AB}\)</span> with dummy variable coding</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>not A</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>B</td>
<td>1,1,1</td>
<td>0,1,0</td>
</tr>
<tr class="even">
<td>not B</td>
<td>1,0,0</td>
<td>0,0,0</td>
</tr>
</tbody>
</table>
<p>or variables <span class="math inline">\(z_A, z_B, z_{AB}\)</span> with effect coding</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>not A</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>B</td>
<td>+1,+1,+1</td>
<td>-1,+1,-1</td>
</tr>
<tr class="even">
<td>not B</td>
<td>+1,-1,-1</td>
<td>-1,-1,+1</td>
</tr>
</tbody>
</table>
<p>If we write <span class="math inline">\(\alpha\)</span> for the coefficients in the first model and <span class="math inline">\(\beta\)</span> for the coefficients in the second model, and include an intercept, we have</p>
<ul>
<li><span class="math inline">\(\alpha_A=1, \alpha_B=1, \alpha_{AB}=1\)</span></li>
<li><span class="math inline">\(\beta_A=3/4, \beta_B=3/4, \beta_{AB}=1/4\)</span></li>
</ul>
<p>There are two reasons for the difference; an interesting one and a boring one. The boring reason is that <span class="math inline">\(1-0=1\)</span> but <span class="math inline">\(1--1=2\)</span>, so the <span class="math inline">\(\beta\)</span>s are half as big as the <span class="math inline">\(\alpha\)</span>s. The interesting reason is that the ‘main effect’ <span class="math inline">\(\alpha_A\)</span> is the difference under ‘not B’, but the main effect <span class="math inline">\(\beta_A\)</span> is averaged over B and not B; it includes some of what <span class="math inline">\(\alpha\)</span> calls the interaction, so it’s bigger and <span class="math inline">\(\beta_{AB}\)</span> is correspondingly smaller. We can think of <span class="math inline">\(\alpha_A\)</span> as the average effect of intervention A in settings where intervention B is naturally absent; <span class="math inline">\(\beta_A\)</span> is the average effect of intervention A in settings where intervention B is naturally present half the time.</p>
<p><em>The regression coefficient is not the only way to express the effect size of an interaction.</em></p>
<p>Now, we know that the test for <span class="math inline">\(\alpha_{AB}=0\)</span> must have the same power as the test for <span class="math inline">\(\beta_{AB}=0\)</span>, because it compares the same two models. What’s a bit less obvious is what happens to the ‘main effects’. Let’s simulate</p>
<pre class="r"><code>x_dummy&lt;-data.frame(a=c(0,0,1,1),b=c(0,1,0,1),ab=c(0,0,0,1))
z_effects&lt;-data.frame(a=c(-1,-1,1,1),b=c(-1,1,-1,1),ab=c(1,-1,-1,1))
i&lt;-rep(1:4,1000)
mu&lt;-c(0,1,1,3)
y&lt;-rnorm(4000,mu[i])
x_model&lt;-lm(y~a+b+ab,data=x_dummy[i,])
z_model&lt;-lm(y~a+b+ab,data=z_effects[i,])
x_noint&lt;-lm(y~a+b,data=x_dummy[i,])
z_noint&lt;-lm(y~a+b,data=z_effects[i,])</code></pre>
<p>First, lets look at the interaction tests</p>
<pre class="r"><code>anova(x_noint, x_model)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: y ~ a + b
## Model 2: y ~ a + b + ab
##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1   3997 4373.2                                  
## 2   3996 4155.7  1    217.42 209.06 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>anova(z_noint,z_model)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: y ~ a + b
## Model 2: y ~ a + b + ab
##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1   3997 4373.2                                  
## 2   3996 4155.7  1    217.42 209.06 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The tests are identical, as they should be.</p>
<p>Now look at the coefficients and their tests</p>
<pre class="r"><code>summary(x_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ a + b + ab, data = x_dummy[i, ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.5432 -0.7034  0.0188  0.7051  3.9701 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.01472    0.03225   0.457    0.648    
## a            1.03193    0.04561  22.627   &lt;2e-16 ***
## b            1.00289    0.04561  21.990   &lt;2e-16 ***
## ab           0.93257    0.06450  14.459   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.02 on 3996 degrees of freedom
## Multiple R-squared:  0.5265, Adjusted R-squared:  0.5261 
## F-statistic:  1481 on 3 and 3996 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>summary(z_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ a + b + ab, data = z_effects[i, ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.5432 -0.7034  0.0188  0.7051  3.9701 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.26528    0.01612   78.47   &lt;2e-16 ***
## a            0.74911    0.01612   46.46   &lt;2e-16 ***
## b            0.73459    0.01612   45.56   &lt;2e-16 ***
## ab           0.23314    0.01612   14.46   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.02 on 3996 degrees of freedom
## Multiple R-squared:  0.5265, Adjusted R-squared:  0.5261 
## F-statistic:  1481 on 3 and 3996 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The <span class="math inline">\(t\)</span>-statistics for the interaction terms, again, are the same. In each model, the <span class="math inline">\(t\)</span>-statistics for the main effects are larger than for the interaction effect. In the dummy-variable model, the coefficients are the same, but the standard error is larger for the main effects. In the effects-coding model, the standard errors are all the same, but the coefficient is smaller for the interaction.</p>
<p><em>The regression coefficient is not the only way to express the effect size of an interaction.</em></p>
<p>Finally, look at the coefficients and tests for the no-interaction model. This model is misspecified – the residual variance isn’t constant – but with equal group sizes that doesn’t matter much.</p>
<pre class="r"><code>summary(x_noint)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ a + b, data = x_dummy[i, ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7764 -0.7356  0.0045  0.7083  4.2032 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.21842    0.02865  -7.625 3.04e-14 ***
## a            1.49822    0.03308  45.294  &lt; 2e-16 ***
## b            1.46917    0.03308  44.416  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.046 on 3997 degrees of freedom
## Multiple R-squared:  0.5017, Adjusted R-squared:  0.5015 
## F-statistic:  2012 on 2 and 3997 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>summary(z_noint)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ a + b, data = z_effects[i, ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7764 -0.7356  0.0045  0.7083  4.2032 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.26528    0.01654   76.50   &lt;2e-16 ***
## a            0.74911    0.01654   45.29   &lt;2e-16 ***
## b            0.73459    0.01654   44.42   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.046 on 3997 degrees of freedom
## Multiple R-squared:  0.5017, Adjusted R-squared:  0.5015 
## F-statistic:  2012 on 2 and 3997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Now the interesting reason for differences in coefficients has gone and the <span class="math inline">\(\alpha\)</span>s are just twice the <span class="math inline">\(\beta\)</span>s; there’s no difference in the tests.</p>
<p>If you stick to quantities that have the same meaning under the two parametrisations, you get the same answer. So what pairs of statements about interaction and power describe the simulation setting and have the same meaning?
Working in terms of coefficients isn’t very satisfactory</p>
<ul>
<li><strong>dummy:</strong> if the interaction coefficient is the same size as the main effects you need four times the sample size to detect it</li>
<li><strong>effects:</strong> if the interaction coefficient half as large as the main effects you need four times the sample size to detect it</li>
</ul>
<p>Working in terms of contrasts is a bit better</p>
<ul>
<li><strong>dummy:</strong> if the difference between AB and the average of A and B is the same as the average of A and B, you need four times the sample size to detect it</li>
<li><strong>effects:</strong> if the difference between AB and the average of A and B is the same as the average of A and B, you need four times the sample size to detect it</li>
</ul>
<p>Or in sound bites</p>
<ul>
<li><strong>dummy:</strong> Interactions are hard to detect because the uncertainty is larger than you might expect</li>
<li><strong>effects:</strong> Interactions are hard to detect because the coefficient is smaller than you might expect</li>
</ul>
<p>And, as an update, what value would we need for the top left (AB) mean of <span class="math inline">\(Y\)</span> to have <span class="math inline">\(\beta_{AB}=\beta_A=\beta_B\)</span>?
We would need</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>not A</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>B</td>
<td>4</td>
<td>1</td>
</tr>
<tr class="even">
<td>not B</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>So, that’s what the table looks like when the interaction parameter is the same size as the main-effects parameter under the effects coding.</p>
<p><em>The regression coefficient is not the only way to express the effect size of an interaction.</em></p>

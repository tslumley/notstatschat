---
title: "Why I like the Convolution Theorem"
author: "Thomas Lumley"
date:  2017-03-27
output: html_document
---



<p>The convolution theorem (or theorems: it has versions that some people would call distinct species and other would describe as mere subspecies) is another almost obviously almost true result, this time about asymptotic efficiency. It’s an asymptotic version of the Cramér–Rao bound. </p>
<p>Suppose <span class="math inline">\(\hat\theta\)</span> is an efficient estimator of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\tilde\theta\)</span> is another, not fully efficient, estimator.  The convolution theorem says that if you rule out stupid exceptions, asymptotically <span class="math inline">\(\tilde\theta=\hat\theta+e\)</span> where <span class="math inline">\(e\)</span> is pure noise, independent of <span class="math inline">\(\hat\theta\)</span>.  </p>
<p>The reason that’s almost obvious is that if it weren’t true, there would be some information about <span class="math inline">\(\theta\)</span> in <span class="math inline">\(\tilde\theta-\hat\theta\)</span>, and you could use this information to get a better estimator than <span class="math inline">\(\hat\theta\)</span>, which (by assumption) can’t happen. The stupid exceptions are things like the <a href="http://notstatschat.netlify.com/2015/05/12/superefficiency/">Hodges superefficient estimator</a> that do better at a few values of <span class="math inline">\(\theta\)</span> but much worse at neighbouring values. </p>
<p>In the usual case where everything is asymptotically Normal, </p>
<p><span class="math display">\[\sqrt{n}\begin{pmatrix} \hat\theta-\theta_0\\  
\tilde\theta-\theta_0\end{pmatrix}\stackrel{d}{\to} N\left( 0, \begin{pmatrix} \sigma^2 &amp; \sigma^2\\  
\sigma^2 &amp;\sigma^2+\omega^2\end{pmatrix}\right)\]</span></p>
<p>The interesting part of that equation is the off-diagonal term. It’s the way it is because any other choice would imply the existence of a linear combination of <span class="math inline">\(\hat\theta\)</span> and <span class="math inline">\(\tilde\theta\)</span> with better efficiency than <span class="math inline">\(\hat\theta\)</span>.  Rescaling to correlations, the square of the correlation between <span class="math inline">\(\hat\theta\)</span> and <span class="math inline">\(\tilde\theta\)</span> is the relative efficiency. </p>
<p>I think the convolution theorem is a useful way to think about asymptotic efficiency (and the fact that it has assumptions is a useful reminder that asymptotic efficiency is less elegant than it should be). </p>
<p>Also, more or less by the definition of influence functions, it follows that the squared correlation between the influence functions for <span class="math inline">\(\hat\theta\)</span> and <span class="math inline">\(\tilde\theta\)</span> is also the asymptotic relative efficiency.  That’s nice because in simulations we can evaluate the influence functions at the true parameter value and don’t need to solve the estimating equations iteratively.  </p>
<p>In particular, now I’m looking at relative efficiency of weighted and unweighted logistic regression in the case-control design, I can compute the correlation between the estimating functions without needing the fitted log odds ratio estimates.  That saves iteration, but more importantly it still works if the case and control <span class="math inline">\(X\)</span> distributions completely separate in a minority of the simulations. Now, I could just estimate the two variances at the true parameter value instead (with about as much work) but working with the correlation has made it easier to prove some special cases analytically.  </p>

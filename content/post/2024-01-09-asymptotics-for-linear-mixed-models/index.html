---
title: Asymptotics for linear mixed models
author: Thomas Lumley
date: '2024-01-09'
slug: asymptotics-for-linear-mixed-models
categories: []
tags: []
---



<p><em>Attention Conservation Notice: I haven’t seen this anywhere, but it’s probably well known in some circles</em></p>
<p>Suppose you have a (parametric, Normal) linear mixed model
<span class="math display">\[Y=X\beta+Zb+\epsilon\]</span>
where <span class="math inline">\(\epsilon\)</span> are iid <span class="math inline">\(N(0,\sigma^2)\)</span> and <span class="math inline">\(b\)</span> are <span class="math inline">\(N(0, \sigma^2V(\theta))\)</span>. Write <span class="math inline">\(\Xi\)</span> for the marginal covariance matrix of <span class="math inline">\(Y\)</span>
<span class="math display">\[\Xi = \mathrm{cov}[Y]=\sigma^2(I+Z^TVZ)\]</span></p>
<p>The loglikelihood can be written
<span class="math display">\[\ell=-\frac{1}{2}\log\left|\Xi\right|-\frac{1}{2}\sum_{i,j} (Y_i-X_i\beta)^T\Xi^{-1}(Y_j-X_j\beta).\]</span></p>
<p>We might want to treat this as a <span class="math inline">\(M\)</span>-estimation problem and use the asymptotic behaviour of <span class="math inline">\(\ell(\beta,\sigma^2,\theta)\)</span> plus the smoothness of <span class="math inline">\(\ell\)</span> to deduce the asymptotic behaviour of the parameter estimates. A complication is that the loglikelihood isn’t a sum and most of our limit theorems are about sums.</p>
<p>The loglikelihood is, however, a quadratic form in Gaussian variables. This has a known distribution: it’s a linear combination of <span class="math inline">\(n\)</span> non-central <span class="math inline">\(\chi^2_1\)</span> variables, where the coefficients of the linear combination are the eigenvalues of <span class="math display">\[J(\beta,\sigma^2,\theta)=\Xi^{-1}(\beta,\sigma,\theta)\Xi(\beta_0,\sigma_0,\theta_0).\]</span> Here the first <span class="math inline">\(\Xi\)</span> is the matrix in the middle of the quadratic form (evaluated at the <strong>current</strong> parameters) and the second is the actual covariance matrix of <span class="math inline">\(Y-X\beta\)</span> (evaluated at the <strong>true</strong> parameters).</p>
<p>Since <span class="math inline">\(\ell\)</span> is a sum of independent non-central <span class="math inline">\(\chi^2\)</span>, it is asymptotically Normal as long as the coefficients and non-centrality parameters satisfy a Lindeberg condition. This condition will certainly be satisfied if the parameter values are close enough to the true values; at the true values <span class="math inline">\(J\)</span> is the identity matrix.</p>
<p>If <span class="math inline">\(\ell\)</span> is asymptotically Normally distributed and the true values of the parameters aren’t at the boundary of the parameter space, we can hope to use the smoothness of <span class="math inline">\(\ell\)</span> to derive asymptotic Normality of the parameter estimates in the usual sort of way.</p>
<p>The assumption that the model is correctly specified isn’t really needed; if the data come from a mixed model different from the one being fitted, the loglikelihood will still be a quadratic form in <span class="math inline">\(Y-\mu\)</span>. We’ll need to worry more about the Lindeberg condition on the non-centrality parameters and eigenvalues since the matrix <span class="math inline">\(J\)</span> need not be all that close to the identity. We’ll also have the usual issues about what even do the parameters mean if the model is misspecified.</p>
<p>Interestingly, the Normality assumption seems much more important. It’s hard to say anything about quadratic forms in non-Normal variables, because decorrelating non-Normal variables need not leave them even approximately independent. More theoretical machinery seems to be needed to tackle non-Normal <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(b\)</span>.</p>

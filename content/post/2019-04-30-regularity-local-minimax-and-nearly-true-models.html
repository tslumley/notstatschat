---
title:  Local asymptotic minimax, and nearly-true models
author: Thomas Lumley
date: '2019-04-30'
slug: regularity-local-minimax-and-nearly-true-models
categories: []
tags: []
---



<p>I’ve written a <a href="https://notstatschat.rbind.io/2014/10/25/semiparametric-efficiency-and-nearly-true-models/">bunch</a> of <a href="https://notstatschat.rbind.io/2016/01/13/another-view-of-thenearly-true-model/">times</a> about nearly-true models. The idea is that you have some regression model for <span class="math inline">\(Y|X\)</span> you’re trying to fit with data from a two-phase sample with known sampling probabilities <span class="math inline">\(\pi_i\)</span> for individual <span class="math inline">\(i\)</span>. You know <span class="math inline">\(Y\)</span> and some auxililary variables <span class="math inline">\(A\)</span> for everyone, but you know <span class="math inline">\(X\)</span> only for the subsample. If you had complete data, you’d fit a particular parametric model for <span class="math inline">\(Y|X\)</span>, with parameters <span class="math inline">\(\theta\)</span> you’re interested in and nuisance parameters <span class="math inline">\(\eta\)</span>, call it <span class="math inline">\(P_{\theta,\eta}\)</span>.</p>
<p>You can assume</p>
<ul>
<li><strong>the sampling model:</strong> just that the sampling probabilities are known</li>
<li><strong>the sampling+outcome model:</strong> that, in addition, <span class="math inline">\(Y|X\)</span> truly follows <span class="math inline">\(P_{\theta,\eta}\)</span></li>
</ul>
<p>Under only the sampling model, the best estimator of <span class="math inline">\(\theta\)</span> is the optimal AIPW estimator <span class="math inline">\(\hat\theta_w\)</span> : it weights the observations where we know <span class="math inline">\(X\)</span> by probabilities based on <span class="math inline">\(\pi_i\)</span> but adjusted using <span class="math inline">\(A\)</span>. Under the sampling+outcome models you can do better, and we’ll write <span class="math inline">\(\hat\theta_e\)</span> for the semiparametric-efficient estimator.</p>
<p>I’m interested in how the estimators compare when the outcome model is nearly true. That is, the data actually come from a model <span class="math inline">\(Q\)</span> which is close to <span class="math inline">\(P_{\theta,\eta}\)</span> for some <span class="math inline">\((\theta,\eta)\)</span>, close enough that you wouldn’t be able to tell the difference given the amount of data you have. As <span class="math inline">\(n\)</span> increases, you can tell the difference better, so <span class="math inline">\(Q\)</span> needs to move closer: we have a sequence <span class="math inline">\(Q_n\)</span> contiguous to the ‘nearly true’ <span class="math inline">\(P_n=P_{\theta_0,\eta_0}\)</span>.</p>
<p>I’m defining the ‘true’ value of <span class="math inline">\(\theta\)</span> as the value you would estimate with complete data, where the two estimators agree. What I’ve shown in the past is that (for large enough <span class="math inline">\(n\)</span>) you can always find <span class="math inline">\(Q_n\)</span> where the outcome model can’t be reliably rejected but where <span class="math inline">\(\hat\theta_w\)</span> has higher mean squared error than <span class="math inline">\(\hat\theta_e\)</span></p>
<p>The standard theoretical result in this direction is the local asymptotic minimax theorem. Here’s the version from van der Vaart &amp; Wellner’s book</p>
<p><strong>3.11.5 Theorem (Minimax theorem)</strong>. <em>Let the sequence of experiments <span class="math inline">\((X_n,{\cal A}_n, P_{n,h} :h\in H)\)</span> be asymptotically normal and the sequence of parameters <span class="math inline">\(\kappa_n(h)\)</span> be regular. Suppose a tight, Borel measurable Gaussian element <span class="math inline">\(G\)</span>, as in the statement of the convolution theorem, exists. Then for every asymptotically <span class="math inline">\(B&#39;\)</span>-measurable estimator sequence <span class="math inline">\(T_n\)</span> and <span class="math inline">\(\tau(B&#39;)\)</span>-subconvex function <span class="math inline">\(\ell\)</span>, <span class="math display">\[\sup_{I\subset H} \liminf_{n\to\infty}\sup_{h\in I}E_{h\star}[r_n(T_n-\kappa_n(h))]\geq E[\ell(G)]\]</span> Here the first supremum is taken over all finite subsets <span class="math inline">\(I\)</span> of <span class="math inline">\(H\)</span>.</em></p>
<p>That might need a little translation. <span class="math inline">\(H\)</span> is the model space, which is a (possibly infinite-dimensional) vector space. <span class="math inline">\(P_{n,h}\)</span> is a way to define a distribution near some distribution <span class="math inline">\(P\)</span>; think of it as being different by <span class="math inline">\(h/\sqrt{n}\)</span>. In our setting <span class="math inline">\(r_n=\sqrt{n}\)</span>; it says how fast everything needs to scale to be just interesting different. The parameters <span class="math inline">\(\kappa_n(h)\)</span> are the parameters you’re interested in: in our case, <span class="math inline">\(\kappa_n(h)\)</span> is the ‘true’ value of <span class="math inline">\(\theta\)</span> for the distribution <span class="math inline">\(P_{n,h}\)</span>. We can tiptoe past the measurability assumption, because we’re not Jon Wellner, and <em><span class="math inline">\(\tau(B&#39;)\)</span>-subconvex function</em> in our case is just the Euclidean squared error – the point of having more generality in the theorem is to show that the result isn’t something weird about squared-error loss. <span class="math inline">\(E_h\)</span> is the expectation under <span class="math inline">\(P_{n,h}\)</span> and <span class="math inline">\(E\)</span> is the expectation under the limiting <span class="math inline">\(P\)</span>. Finally, <span class="math inline">\(G\)</span> is the limiting Normal distribution of the efficient estimator in whatever model we’re working in.</p>
<p>If we were working in a parametric model you could take <span class="math inline">\(I\)</span> to be a ball of some radius <span class="math inline">\(\delta\)</span>, and the result would then say that for any <span class="math inline">\(\delta_n\to\infty\)</span> there’s some sequence of <span class="math inline">\(P_{n,h}\)</span> at distances <span class="math inline">\(\delta_n/\sqrt{n}\)</span> from <span class="math inline">\(P\)</span> where the mean squared error of <span class="math inline">\(T_n\)</span> is asymptotically no better than it is for the efficient estimator. That is, <span class="math inline">\(T_n\)</span> can’t be better than the efficient estimator uniformly even over very small neighbourhoods of a point. For technical reasons you can’t use balls of small radius as small neighbourhoods in the infinite-dimensional case, but the idea is the same.</p>
<p>We’d normally use the local asymptotic minimax theorem with the sampling+outcome model being <span class="math inline">\(H\)</span>, in which case it would show us that no estimator <span class="math inline">\(T_n\)</span> could beat <span class="math inline">\(\hat\theta_e\)</span>. Instead, we’re going to use it with the sampling model being <span class="math inline">\(H\)</span> (or some well-behaved large submodel that I won’t try to specify here). The efficient estimator is <span class="math inline">\(\hat\theta_w\)</span>, and <span class="math inline">\(\hat\theta_e\)</span> is our alternative estimator <span class="math inline">\(T_n\)</span>, and we’re working at a point <span class="math inline">\(P\)</span> where the sampling+outcome model is true.</p>
<p>The theorem now talks about nearby distributions <span class="math inline">\(P_{n,h}\)</span> where the sampling model is true but the outcome model isn’t. There are sequences of <span class="math inline">\(P_{n,h}\)</span> converging to <span class="math inline">\(P\)</span> where <span class="math inline">\(\hat\theta_e\)</span> (ie, <span class="math inline">\(T_n\)</span>) doesn’t beat the weighted estimator <span class="math inline">\(\hat\theta_w\)</span>. The efficiency advantage of <span class="math inline">\(\hat\theta_e\)</span> doesn’t generalise even a very little way away from where the outcome model is true.</p>
<p>That’s less persuasive (I think) than my construction. First, it doesn’t show <span class="math inline">\(\hat\theta_w\)</span> is <em>better</em>, just that it’s no worse. Second, the distance between the true and nearly-true model is <span class="math inline">\(\delta_n/\sqrt{n}\)</span> for <span class="math inline">\(\delta_n\)</span> potentially diverging to infinity. In my construction, we reach equal mean squared error at an explicit, finite <span class="math inline">\(\delta\)</span>, and it keeps getting worse for larger <span class="math inline">\(\delta\)</span>.</p>
<p>The reason I can do better is because of regularity. The full power of the local asymptotic minimax theorem is needed for estimators with unsmooth behaviour as a function of <span class="math inline">\(h\)</span>: these can be silly counterexamples like <a href="https://notstatschat.rbind.io/2015/05/12/superefficiency/">Hodges’s superefficient estimator</a> or useful ideas like the lasso, or something in between, like a regression estimator that adjusts only for statistically significant confounders.</p>
<p>A compromise estimator based on testing goodness of fit of the outcome model could mitigate the breakdown of <span class="math inline">\(\hat\theta_e\)</span>. It still couldn’t do uniformly <em>better</em> than <span class="math inline">\(\hat\theta_w\)</span> when the model was only nearly true – the local asymptotic minimax theorem guarantees that. It’s concievable that it could do well enough to be preferable.</p>

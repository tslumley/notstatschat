---
title: Simulation and CLT
author: Thomas Lumley
date: '2026-01-05'
slug: simulation-and-clt
categories: []
tags: []
---



<p>Riffing off Andrew Gelman’s (correct) advice to <a href="https://statmodeling.stat.columbia.edu/2026/01/04/when-in-doubt-in-teaching-and-in-research-do-a-simulation-on-the-computer/">just simulate</a>, in both teaching and research, suppose I have a new robust location estimator. For a data set <span class="math inline">\(X_1,\dots, X_n\)</span>, I want to find the densest half of the distribution and take some location summary of it.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Let’s say I want the mean. That is, I want to define a 50% subset of the data by</p>
<pre><code>function(x) {
	s&lt;-sort(x)
	n&lt;-length(s)
	halfn&lt;- n%/%2
	i&lt;-which.min(sapply(1:halfn, function(i) diff(range(s[i:(i+halfn)]))))
	mean(s[i:(i+halfn)])
}</code></pre>
<p>though with better handling of ties and missing values in production code.</p>
<p>This is pretty clearly consistent for the mean of the densest half of the true distribution. It’s going to be quite a bit harder<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> to prove that it has a Normal distribution asymptotically, but we can simulate it and check. It’s an average, at least.</p>
<p>We might give the function a name, such as <code>shorthalf</code></p>
<pre class="r"><code>shorthalf&lt;-function(x) {
	s&lt;-sort(x)
	n&lt;-length(s)
	halfn&lt;- n%/%2
	i&lt;-which.min(sapply(1:halfn, function(i) diff(range(s[i:(i+halfn)]))))
	mean(s[i:(i+halfn)])
}</code></pre>
<p>and generate a sample from it for, say, Normal <span class="math inline">\(X\)</span></p>
<pre class="r"><code>set.seed(2025-1-6)
a_sample &lt;- replicate(1000, {x&lt;-rnorm(100); shorthalf(x)})</code></pre>
<p>Now we could plot a histogram</p>
<pre class="r"><code>hist(a_sample,breaks=50,prob=TRUE)
s&lt;-sd(a_sample)
curve(dnorm(x,s=s),add=TRUE, col=&quot;red&quot;,lwd=2)</code></pre>
<p><img src="staticunnamed-chunk-3-1.png" width="90%" />
or a quantile-quantile plot</p>
<pre class="r"><code>par(pty=&quot;s&quot;)
qqnorm(a_sample)</code></pre>
<p><img src="staticunnamed-chunk-4-1.png" width="90%" /></p>
<p>Not bad. Maybe try a larger sample?</p>
<pre class="r"><code>b_sample &lt;- replicate(10000, {x&lt;-rnorm(1000); shorthalf(x)})</code></pre>
<p>and the plots</p>
<pre class="r"><code>hist(b_sample,breaks=50,prob=TRUE)
s&lt;-sd(b_sample)
curve(dnorm(x,s=s),add=TRUE, col=&quot;red&quot;,lwd=2)</code></pre>
<p><img src="staticunnamed-chunk-6-1.png" width="90%" /></p>
<pre class="r"><code>par(pty=&quot;s&quot;)
qqnorm(b_sample)</code></pre>
<p><img src="staticunnamed-chunk-6-2.png" width="90%" /></p>
<p>And maybe see what a Shapiro-Wilk test says (it can’t handle more than 5000 points, because that’s as far as Patrick Royston studied the approximation to the null distribution)</p>
<pre class="r"><code>shapiro.test(b_sample[1:5000])</code></pre>
<pre><code>## 
## 	Shapiro-Wilk normality test
## 
## data:  b_sample[1:5000]
## W = 0.99954, p-value = 0.2736</code></pre>
<p>One thing you might notice is the inefficiency</p>
<pre class="r"><code>var(a_sample)</code></pre>
<pre><code>## [1] 0.06977316</code></pre>
<pre class="r"><code>var(b_sample)</code></pre>
<pre><code>## [1] 0.01301342</code></pre>
<p>The variances for the mean would be 0.01 and 0.001. For the median they would be a bit larger, but they’d still scale by close to a factor of ten:</p>
<pre class="r"><code>var(replicate(1000, {x&lt;-rnorm(100); median(x)}))</code></pre>
<pre><code>## [1] 0.01519022</code></pre>
<pre class="r"><code>var(replicate(10000, {x&lt;-rnorm(1000); median(x)}))</code></pre>
<pre><code>## [1] 0.001616641</code></pre>
<p>Our new estimator seems to be massively less efficient than the median at Normal distributions, which is a bit disappointing.</p>
<p>At this point you might be wondering if anyone has studied this estimator before. Indeed they have! It is Tukey’s <em>shorth</em>, and it is known to have really low efficiency at the Normal. In fact, it has spread of order <span class="math inline">\(n^{-1/3}\)</span> (variance of order <span class="math inline">\(n^{-2/3}\)</span>), in contrast to the <span class="math inline">\(n^{-1/2}\)</span> of reasonable estimators, and is not asymptotically Normal.</p>
<p>The scaling of the variance with sample size is clearly a better fit for the cube-root asymptotics</p>
<pre class="r"><code>var(a_sample)*(100)</code></pre>
<pre><code>## [1] 6.977316</code></pre>
<pre class="r"><code>var(b_sample)*(1000)</code></pre>
<pre><code>## [1] 13.01342</code></pre>
<pre class="r"><code>var(a_sample)*(100)^(2/3)</code></pre>
<pre><code>## [1] 1.503217</code></pre>
<pre class="r"><code>var(b_sample)*(1000)^(2/3)</code></pre>
<pre><code>## [1] 1.301342</code></pre>
<p>“But it <em>looks</em> asymptotically Normal!”, you respond. Yes, it does.</p>
<p>Rather than looking at the Normal qq-plot, we can look at the qq-plot for the correct asymptotic distribution. Chernoff’s distribution is the ‘canonical’ cube-root limiting distribution in much the same way that the Gaussian is the limiting distribution for square-root asymptotics. It’s fairly close to <span class="math inline">\(N(0,.52^2)\)</span>, but has lighter tails.</p>
<p>Chernoff’s distribution arises in the asymptotics as the distribution of the location of the maximum of Brownian motion minus a quadratic drift term. In the tails, the density is approximated by
<span class="math display">\[f(z)\sim a|z|\exp\{ -\frac{2}{3}|z|^3+b|z|\},\]</span>
where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are unpleasant but precisely described positive constants, so we can see the tail is exponentially lighter than a Normal. Computing the density near zero is … not entirely straightforward.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Fortunately, there’s a <code>ChernoffDist</code> package.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>The <code>qChern</code> quantile function only works down to <span class="math inline">\(10^{-3}\)</span> out of the box, because the starting values for its zero-finding break down, but we can fix that</p>
<pre class="r"><code>qChern&lt;-function (p) 
{
    return(stats::uniroot(function(x) {
        ChernoffDist::pChern(x) - p
    }, c(stats::qnorm(p, sd = 0.52) - 0.3, stats::qnorm(p, sd = 0.52) + 
        0.31), tol = 1e-10)$root)
}</code></pre>
<pre class="r"><code>qch&lt;-sapply(ppoints(10000), qChern)</code></pre>
<p>and produce a qq-plot to compare to the Normal one</p>
<pre class="r"><code>par(pty=&quot;s&quot;)
qqplot(qch,b_sample)</code></pre>
<p><img src="staticunnamed-chunk-13-1.png" width="90%" /></p>
<pre class="r"><code>qqnorm(b_sample)</code></pre>
<p><img src="staticunnamed-chunk-13-2.png" width="90%" /></p>
<p>a difference which is robustly unimpressive.</p>
<p>We can look a bit more at the efficiency by adding another sample size</p>
<pre class="r"><code>c_sample &lt;- replicate(1000, {x&lt;-rnorm(10000); shorthalf(x)})
var(c_sample)/var(b_sample)</code></pre>
<pre><code>## [1] 0.2001303</code></pre>
<pre class="r"><code>var(b_sample)/var(a_sample)</code></pre>
<pre><code>## [1] 0.1865103</code></pre>
<pre class="r"><code>0.1^(2/3)</code></pre>
<pre><code>## [1] 0.2154435</code></pre>
<p>which is still scaling nicely at the cube-root rate.</p>
<p>You might say that my simulations were all for a Normal, and maybe things would be different if <span class="math inline">\(X\)</span> had a <span class="math inline">\(t_{3.2}\)</span> distribution or a Gamma distribution or something, and to some extent that’s right. If <span class="math inline">\(X\)</span> is skewed it takes quite large samples for the skewness to disappear from the distribution of the shorth. But the comparison between <code>qqnorm</code> and <code>qq_actually_chernoff</code> doesn’t depend on the distribution of <span class="math inline">\(X\)</span>; the two plots will be almost identical for any raw data distribution.</p>
<p>So maybe you don’t care about the difference between Chernoff’s distribution and an approximating Normal, but you probably would care about the efficiency, and you might need to know about the asymptotic rate if you want to do, eg, sample size calculations. And simulations may tell you this more quickly than finding the theoretical analysis of the shorth.</p>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>if you know the punch line here then you are old enough to keep it to yourself<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>for various reasons<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>according to <a href="https://doi.org/10.1198%2F10618600152627997">Piet Groeneboom</a><a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>though it could do with a bit of work on vectorisation, if anyone is interested<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

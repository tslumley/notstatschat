---
title: "Chebyshev’s inequality and `UCL’"
author: "Thomas Lumley"
date: 2018-03-15
output: html_document
---



<p>Chebyshev’s inequality (or any of the other transliterations of Чебышёв) is a simple bound on the proportion of a distribution that can be far from the mean. The Wikipedia page, on the other hand, isn’t simple. I’m hoping this will be more readable.</p>
<p>We have a random quantity <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, and – knowing nothing else – we want to say something about the probability that <span class="math inline">\(X-\mu\)</span> is large.  Since we know nothing else, we need to find the largest possible value of <span class="math inline">\(\Pr(|X-\mu|\geq d)\)</span> for any distribution with that mean and variance. </p>
<p>It’s easier to think about the problem the other way: fix <span class="math inline">\(\Pr(|X-\mu|\geq d)\)</span>, and ask how small we can make the variance.  So, how can we change the distribution to reduce the variance without changing <span class="math inline">\(\Pr(|X-\mu|\geq d)\)</span>?  For any point further than <span class="math inline">\(d\)</span> away we can move it to distance <span class="math inline">\(d\)</span> away. It still has distance at least <span class="math inline">\(d\)</span>, but the variance is smaller.  For any point closer than <span class="math inline">\(d\)</span> we can move it all the way to the mean. It’s still closer than <span class="math inline">\(d\)</span>, and the variance is smaller.</p>
<p>So, the worst-case distribution has all its probability at <span class="math inline">\(\mu-d\)</span>, <span class="math inline">\(\mu\)</span>, or <span class="math inline">\(\mu+d\)</span>.  Write <span class="math inline">\(p/2\)</span> for the probability that <span class="math inline">\(X=\mu-d\)</span>. This  must be the same as the probability that <span class="math inline">\(X=\mu+d\)</span>, or <span class="math inline">\(\mu\)</span> wouldn’t be the mean.  By a straightforward calculation the variance of this worst-case distribution is <span class="math inline">\(\sigma^2=pd^2\)</span>.  So, <span class="math inline">\(p=\sigma^2/d^2\)</span> for the worst-case distribution, and <span class="math inline">\(p\leq \sigma^2/d^2\)</span> always.</p>
<p>In environmental monitoring there’s an approach to upper confidence limits for a mean based on this: the variance of the mean of <span class="math inline">\(n\)</span> observations is <span class="math inline">\(\sigma^2/n\)</span>, and the probability of being more than <span class="math inline">\(d\)</span> units away from the mean is bounded by <span class="math inline">\(\sigma^2/nd^2\)</span>.  The problem, where this is used in environmental monitoring, is that you don’t know <span class="math inline">\(\sigma^2\)</span>.  You’ve got an estimate based on the data, but this estimate is going to be unreliable in precisely the situations where you’d need a conservative, worst-case interval.  The approach is ok if you’ve taken enough samples to see the bad bits of the pollution distribution, but it’s not very helpful if you don’t know whether you have.</p>

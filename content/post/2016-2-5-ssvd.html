---
title: "Stochastic SVD"
author: "Thomas Lumley"
date: 2016-02-05
output: html_document
---



<p>Suppose you have an <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(A\)</span> of rank <span class="math inline">\(k\)</span>. If <span class="math inline">\(\Omega\)</span> is an <span class="math inline">\(n\times k\)</span> matrix with iid standard Gaussian entries, then <span class="math inline">\(\Omega\)</span> will have rank <span class="math inline">\(k\)</span> with probability 1,  <span class="math inline">\(A\Omega\)</span> will have rank <span class="math inline">\(k\)</span> with probability 1, and so <span class="math inline">\(A\Omega\)</span> spans the range of <span class="math inline">\(A\)</span>. That’s all easy.</p>
<p>More impressively, if <span class="math inline">\(A=\tilde A+\epsilon\)</span> where <span class="math inline">\(\tilde A\)</span> has rank <span class="math inline">\(k\)</span> and <span class="math inline">\(\epsilon\)</span> has small norm, and if <span class="math inline">\(\Omega\)</span> has <span class="math inline">\(k+p\)</span> columns, <span class="math inline">\(A\Omega\)</span> spans the range of <span class="math inline">\(\tilde A\)</span> with high probability, for surprisingly small values of <span class="math inline">\(p\)</span>.  If <span class="math inline">\(Q\)</span> comes from a <span class="math inline">\(QR\)</span> decomposition of <span class="math inline">\(A\Omega\)</span>, then <span class="math inline">\(Q^TA\)</span> has approximately the same <span class="math inline">\(k\)</span> largest singular values as <span class="math inline">\(A\)</span> (or, equivalently, as <span class="math inline">\(\tilde A\)</span>). </p>
<p>The same trick works with lots of other random <span class="math inline">\(\Omega\)</span>, for fixed <span class="math inline">\(p\)</span>. If we are prepared to take <span class="math inline">\(p=\log k\)</span> it even works for a ‘structured’ random <span class="math inline">\(\Omega=SHR\)</span> where <span class="math inline">\(R\)</span> applies random signs to each row, <span class="math inline">\(H\)</span> does the Hadamard Transform, and <span class="math inline">\(S\)</span> takes a random sample of <span class="math inline">\(k\log k\)</span> rows of a matrix.  The point of this choice of <span class="math inline">\(\Omega\)</span> is that <span class="math inline">\(\Omega A\)</span> can be computed in <span class="math inline">\(mn\log n\)</span> time  (with a small constant, and for any <span class="math inline">\(k\)</span>) using the Fast Hadamard Transform, rather than the <span class="math inline">\(mnk\)</span> time of explicit matrix multiplication. </p>
<p>The reference you want is <a href="http://arxiv.org/abs/0909.4061">here</a>: “Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions” by Halko, Martinsson, and Tropp. </p>
<p>One way to think about what’s happening is a combination of <a href="http://notstatschat.tumblr.com/post/129047372011/high-dimensional-space-is-big">“Space is Big. Really Big.”</a> with a version of the Law of Large Numbers. The columns of <span class="math inline">\(A\)</span> are <span class="math inline">\(n\)</span> points in <span class="math inline">\(m\)</span>-dimensional space, and if <span class="math inline">\(m\gg\log n\)</span> they are really sparse. Because they are really sparse, capturing one eigenvector of <span class="math inline">\(A\)</span> is pretty much independent of capturing another one. <span class="math inline">\(\Omega\)</span> doesn’t have any preferences for whether it captures eigenvectors with large or small eigenvalues, but <span class="math inline">\(A\Omega\)</span> magnifies the larger ones. As the paper notes, multiplying by <span class="math inline">\((AA^T)^q\)</span> for some small <span class="math inline">\(q\)</span> improves things even further.  If you only took <span class="math inline">\(k\)</span> dimensions you’d still have a good chance of missing some of the <span class="math inline">\(k\)</span> main eigenvectors, but using <span class="math inline">\(k+p\)</span> dimensions decreases that chance – more or less exponentially in <span class="math inline">\(p\)</span> because independence. <waves hands> The actual proofs, of course, are more complicated and use some fairly deep facts.</p>
<p>The stochastic SVD is no faster on average than Lanczos-type algorithms, but it’s <em>enormously</em> easier to program correctly and comes with simple probabilistic error bounds that are known to be sharp. As with the Lanczos-type algorithms, it can be made much faster if <span class="math inline">\(A\)</span> is sparse or otherwise structured so that multiplication by <span class="math inline">\(A\)</span> is fast.  The stochastic algorithm is also easier to parallelize and can be modified to scan <span class="math inline">\(A\)</span> only a few times, or even only once. </p>

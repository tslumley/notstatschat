---
title: "(high-dimensional) Space is Big."
author: "Thomas Lumley"
date: 2015-09-14
output: html_document
---



<p>Space is big. Really big. You just won’t believe how vastly, hugely, mind-bogglingly big it is. I mean, you may think it’s a long way down the road to the chemist, but that’s just peanuts to space. <em>Hitchhikers Guide to the Galaxy</em></p>
<p>There’s a <a href="https://gist.github.com/tslumley/373e3759a766af106a4e">simple simulation</a> that I used in stat computing class last year and for Rob Hyndman’s working group last week. Simulate data uniformly on a <span class="math inline">\(p\)</span>-dimensional hypercube <span class="math inline">\([0,\,1]^p\)</span> and compute nearest-neighbour distances. </p>
<p>Obviously these distances increase with <span class="math inline">\(p\)</span>, since the squared Euclidean distance is a sum of non-negative terms. The mean distance between two random points is <span class="math inline">\(\sqrt{p}/3\)</span>.  How does this compare to the nearest-neighbour distance?</p>
<pre><code>&gt; x1&lt;-runif(5e4)  
&gt; x2&lt;-matrix(runif(5e4*2),ncol=2)  
&gt; x10&lt;-matrix(runif(5e4*10),ncol=10)  
&gt; x100&lt;-matrix(runif(5e4*100),ncol=100)  
  
system.time(d1&lt;-knn.dist(x1,k=1))  
   user  system elapsed  
  0.044   0.002   0.045  
&gt; system.time(d2&lt;-knn.dist(x2,k=1))  
   user  system elapsed  
  0.066   0.002   0.088  
&gt; system.time(d10&lt;-knn.dist(x10,k=1))  
   user  system elapsed  
  4.005   0.010   3.993  
&gt; system.time(d100&lt;-knn.dist(x100,k=1))  
   user  system elapsed  
 705.281   0.996 702.481  
&gt; mean(d1/sqrt(1)/(1/3))  
 [1] 2.994883e-05  
&gt; mean(d2/sqrt(2)/(1/3))  
 [1] 0.004764815  
&gt; mean(d10/sqrt(10)/(1/3))  
[1] 0.306989  
&gt; mean(d100/sqrt(100)/(1/3))  
[1] 0.9269603</code></pre>
<p>The typical nearest-neighbour distance for 50,000 uniform points in <span class="math inline">\([0,\,1]\)</span> is 0.00003 times the mean interpoint distance, which is sort of what you’d expect. In <span class="math inline">\([0,\,1]^2\)</span> it’s 0.005 times the mean distance, which is not too surprising. </p>
<p>In <span class="math inline">\([0,\,1]^{100}\)</span>, though, the nearest-neighbour distance is over 90% of the mean distance. If you haven’t seen this sort of computation before, that seems outrageous. Roughly speaking, in 100-dimensional space, if you have 50,000 points in a unit cube they are all about equidistant from each other!</p>
<p>If you think of the interpoint distance as a sum, that makes sense in terms of the Law of Large Numbers and Central Limit Theorem. The squared Euclidean distance is a sum of <span class="math inline">\(p\)</span> approximately independent things, so it should be close to its expected value; differing from it by a relative amount <span class="math inline">\(O_p(1/\sqrt{p})\)</span>. If we have <span class="math inline">\(n\)</span> independent random variables like that, and they don’t have heavier tails than a Gaussian (which they  don’t), the worst departure from the expected value should scale like <span class="math display">\[O_p\left(\sqrt{\frac{\log n}{p}}\right).\]</span> In other words, nearest-neighbours can’t be very much closer than the average interpoint distance.</p>
<p>Many ‘black-box’ prediction methods are based on using outcome values from nearby points to predict outcome values for new points. This can’t work in high-dimensional space; there are no nearby points. </p>
<p>In a sense, everyone knows this. Statisticians call it the ‘curse of dimensionality’, and modern sparse regression methods are aimed at bypassing it by not working in high-dimensional spaces. It’s still more dramatic that most people realise, and it’s hard for our intuition to cope, to think of a high-dimensional unit cube or sphere and ‘believe how vastly, hugely, mind-bogglingly big it is’.</p>

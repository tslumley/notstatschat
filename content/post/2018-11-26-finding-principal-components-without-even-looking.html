---
title: Finding principal components without even looking?
author: Thomas Lumley
date: '2018-11-26'
slug: finding-principal-components-without-even-looking
categories: []
tags: []
---



<p>Via <a href="https://www.scottaaronson.com/blog/">Scott Aaronson’s blog</a> I found an <a href="https://arxiv.org/abs/1811.00414">arXiv abstract</a> and then an early paper (<a href="https://www.math.cmu.edu/~af1p/Texfiles/SVD.pdf">PDF</a>) about doing singular value decomposition of an <span class="math inline">\(m\times n\)</span> matrix in less than <span class="math inline">\(O(mn)\)</span> time. That is, you could estimate population structure with principal components of a genotype matrix or work out <a href="https://notstatschat.rbind.io/2016/09/27/large-quadratic-forms/">tail probabilities for a quadratic-form-based test</a> in less time than it takes to actually look at the data. That’s obviously impossible, and so that’s not what the paper actually says.</p>
<p>You <strong>can</strong> actually estimate the first <span class="math inline">\(k\)</span> singular values and a corresponding rank-<span class="math inline">\(k\)</span> approximation to the matrix in that sort of time, under an importance-sampling assumption. It looks like you should be able to compute <span class="math inline">\(k\)</span> population-structure principal components for <span class="math inline">\(G\)</span> SNPs and <span class="math inline">\(N\)</span> people in <span class="math inline">\(O(Nk^2)\)</span> time, and compute <span class="math inline">\(k\)</span> eigenvalues for the SKAT test in <span class="math inline">\(O(G+N+k^3)\)</span> time, both of which grow more slowly than the size of the data. Let’s see how it works.</p>
<p>The key sampling assumption is that you can sample from rows or columns of a matrix <span class="math inline">\(A\)</span> with probability proportional to the <span class="math inline">\(\ell_2\)</span> norm, or at least bounded below by a multiple of the <span class="math inline">\(\ell_2\)</span> norm: there is a <span class="math inline">\(c\)</span> not depending on <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> such that we can find <span class="math inline">\(P_i\)</span> with <span class="math display">\[P_i\geq c\frac{\|A_{.i}\|_2}{\sum_i\|A_{.i}\|_2}.\]</span> The idea is to subsample a matrix of slightly-more-than-<span class="math inline">\(k\)</span> columns with probabilities of this sort, and then rescale by <span class="math inline">\(1/\sqrt{P_i}\)</span> to correct the sampling bias, giving a matrix <span class="math inline">\(S\)</span>, then subsample rows and rescale in the same way to get a small square matrix <span class="math inline">\(W\)</span>. I’ll write <span class="math inline">\(s\)</span> for the number of columns we sample. If you want the singular values, you can just work with the small (<span class="math inline">\(s\times s\)</span>) matrix <span class="math inline">\(W\)</span>; if you want the singular vectors you need the matrix <span class="math inline">\(S\)</span> which is small in one direction (<span class="math inline">\(s\times N\)</span>).</p>
<p>Now, what does the sampling assumption look like for genotype data? When we’re working out population structure by principal components, we have a centered and scaled genotype matrix where each column (SNP) has the same <span class="math inline">\(\ell_2\)</span> norm. That means we can choose the <span class="math inline">\(P_i\)</span> to all be equal, and sampling <span class="math inline">\(k\)</span> columns takes only <span class="math inline">\(O(k)\)</span> time. Sadly, what this means is that the clever new algorithm reduces to the well-known algorithm of using just a random subset of the SNPs to work out population structure. We already know a slightly better fast algorithm – using an evenly spaced subset of SNPs – and we know it needs quite a big subset to be good enough, at least in situations where the confounding by population structure is important. On the other hand, it’s reassuring that the algorithm is sensible and that we don’t already know a much better one for genetics.</p>
<p>The same idea is a bit more promising for the SKAT association tests, because we don’t need as much accuracy there. The computations are a bit slower, because the <span class="math inline">\(\ell_2\)</span>-norm of a SNP column with minor allele frequency <span class="math inline">\(f\)</span> does depend on <span class="math inline">\(f\)</span>: with the standard weights and no adjustment for covariates it’s <span class="math display">\[P_f=\sqrt{f(1-f)/(1-f)^{25}}=\sqrt{f/(1-f)^{24}}.\]</span> With adjustment for covariates the <span class="math inline">\(\ell_2\)</span>-norms will still be pretty close, so we can still use that as an approximation. We can sample proportional to <span class="math inline">\(P_f\)</span>, and we’ve probably already computed the minor allele frequency for other reasons – if not, it takes <span class="math inline">\(MN\)</span> time. It’s probably good enough to use constant sampling probability for the rows, but the exact probabilities aren’t hard. Importantly, this <strong>isn’t</strong> just the same as using a equal-probability subset of SNPs.</p>
<p>Since computing the SKAT test statistic takes <span class="math inline">\(MN\)</span> time, it’s certainly enough to choose the number of columns and rows <span class="math inline">\(s\)</span> small enough that the <span class="math inline">\(s^3\)</span> time for the SVD is no worse that the <span class="math inline">\(MN\)</span> time for the test statistic, giving <span class="math inline">\(s=\sqrt[3]{MN}\)</span>. With <span class="math inline">\(N=M=5000\)</span> that’s about <span class="math inline">\(s=300\)</span>. If that’s not accurate enough, we can try to compete with the <span class="math inline">\(O(MNk)\)</span> time of the <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/gepi.22136">fastSKAT</a> algorithm using stochastic SVD. Worth looking into.</p>

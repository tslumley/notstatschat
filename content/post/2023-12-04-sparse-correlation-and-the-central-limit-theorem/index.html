---
title: Sparse correlation and the Central Limit Theorem
author: Thomas Lumley
date: '2023-12-04'
slug: sparse-correlation-and-the-central-limit-theorem
categories: []
tags: []
---



<p>Back when I was a PhD student working on generalisations of GEE, I was interested in ‘sparse’ correlations, defined by ‘most’ small sets of observations being independent. One way to get this structure is from crossed clustering variables; another is for the basic units in your analysis to be pairs (or larger tuples) of observations. If you drew a graph with the variables as vertices, and connected correlated variables, then two subsets of the variables would be independent of each other if there was no edge between them.</p>
<p>To see what this implies in large samples, consider a balanced incomplete multi-rater experiment with <span class="math inline">\(K\)</span> raters and <span class="math inline">\(D\)</span> objects to be rated, in which each rater rates <span class="math inline">\(1&lt;d\leq D\)</span> of the objects and each object is rated by <span class="math inline">\(1&lt;k\leq K\)</span> of the raters. The number of ratings is <span class="math inline">\(n=Kd=kD\)</span>. Two ratings are connected by an edge in the dependence graph if they share a rating or share a rating. The maximal degree of the graph is <span class="math inline">\(M=k+d-1\)</span>. We are interested in the case where <span class="math inline">\(K\)</span> and <span class="math inline">\(D\)</span> are both large.</p>
<p>Consider a simple parametric random-effects model for the rating <span class="math inline">\(Y_{ij}\)</span> by rater <span class="math inline">\(i\)</span> of object <span class="math inline">\(j\)</span>
<span class="math display">\[Y_{ij}=\mu+a_i+b_j+\epsilon_{ij}\]</span>
where <span class="math inline">\(\mu\)</span> is a fixed constant, <span class="math inline">\(a_i\sim N(0,\sigma^2_a)\)</span>, <span class="math inline">\(b_j\sim N(0,\sigma^2_b)\)</span>, and <span class="math inline">\(\epsilon_{ij}\sim N(0,\sigma^2_\epsilon)\)</span>. The variance of <span class="math inline">\(S_n=\sum_{ij} Y_{ij}\)</span> is
<span class="math display">\[\sigma^2_n=n\sigma^2_\epsilon+Kd^2\sigma^2_a+k^2D\sigma^2_b=n\sigma^2_\epsilon+nd\sigma^2_a+nk\sigma^2_b.\]</span></p>
<p>The ratio <span class="math inline">\(\sigma^2_n/Mn\)</span> will be bounded above and bounded away from zero if <span class="math inline">\(\sigma^2_a\)</span> and <span class="math inline">\(\sigma^2_b\)</span> are both non-zero, and we will have <span class="math inline">\(S_n/n=\bar X_n\stackrel{p}{\to}\mu\)</span> as long as <span class="math inline">\(K,D\to\infty\)</span>.</p>
<p>Now, let’s look at a scaled version of <span class="math inline">\(\bar X_n-\mu\)</span>. We can’t just scale it by <span class="math inline">\(\sqrt{n}\)</span> as in the independent setting, because the variance of <span class="math inline">\(\bar X_n\)</span> is <span class="math inline">\(O(M/n)\)</span> rather than <span class="math inline">\(O(1/n)\)</span>. We can still hope for
<span class="math display">\[\sqrt{\frac{n}{\sigma^2_n}}\left(\bar X_n-\mu\right)\stackrel{d}{\to}N(0,1),\]</span>
since at least it’s the right size. In the simple multi-rater case it certainly works.</p>
<p>Encouragingly, we get the same sort of limit if we have <span class="math inline">\(M\)</span> identical copies of each of <span class="math inline">\(m\)</span> independent items, as another way to get sparse correlation.</p>
<div id="proving-it" class="section level3">
<h3>Proving it?</h3>
<p>The first thing I tried was to see if the moments of <span class="math inline">\(\bar X_n\)</span> did anything helpful. I thought I had a proof, in fact, but it was wrong. The idea does work if you do it with cumulants; Svente Janson did it correctly, ten years earlier, and <a href="https://projecteuclid.org/journals/annals-of-probability/volume-16/issue-1/Normal-Convergence-by-Higher-Semiinvariants-with-Applications-to-Sums-of/10.1214/aop/1176991903.full">published it</a> under the title <em>Normal Convergence by Higher Semiinvariants with Applications to Sums of Dependent Random Variables and Random Graphs</em>. I hadn’t found it in searching, and no-one I talked to at the time had heard of it.</p>
<p>The second thing I tried was to adapt a Central Limit Theorem for random fields by <a href="https://books.google.com.au/books?id=EYO0MNrIT8YC&amp;dq=guyon+book+random+fields&amp;lr=&amp;source=gbs_navlinks_s">Xavier Guyon</a>, which I’d already used in my thesis. The proof uses something called Stein’s Method, and involves a counting bound on ‘close’ pairs of pairs of observations and a long-range weak dependence bound on ‘distant’ pairs of pairs. I could just dump the ‘distant’ part of the proof and make it work for sparse correlation. Success!!</p>
<p>Nicole Mayer-Hamblett and I had a manuscript about asymptotics for sparsely-correlated generalised linear models, with some nice examples, and we got revision requests from a journal and then neither of us had time to do the revision. Then I worked out how to prove <a href="https://notstatschat.rbind.io/2017/07/26/tail-bounds-under-sparse-correlation/">an exponential tail bound</a> for sparse correlation and sent it off to a probability journal. They thought it was maybe ok, but that I needed to look up something called graph-structured dependence in the probability literature. That turned out to be the same Central Limit Theorem proof that I had (only a bit tidier and with an explicit error bound) and had already been published. <a href="https://projecteuclid.org/journals/annals-of-probability/volume-17/issue-4/On-Normal-Approximations-of-Distributions-in-Terms-of-Dependency-Graphs/10.1214/aop/1176991178.full">In 1989</a>. Sigh.</p>
</div>
<div id="the-theorem" class="section level3">
<h3>The Theorem</h3>
<p>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be random variables such that <span class="math inline">\(E[X_i^4]&lt;\infty\)</span>, <span class="math inline">\(E[X_i]=0\)</span>, <span class="math inline">\(\sigma^2=\mathrm{var}[\sum_i X_i]\)</span> and define <span class="math inline">\(S=\sum_i X_i/\sigma\)</span>. Let <span class="math inline">\(M\)</span> be the maximal degree of a dependency graph for the <span class="math inline">\(X_i\)</span>. Then for <span class="math inline">\(Z\)</span> a standard Normal variable
<span class="math display">\[d_W(S,Z)\leq \frac{M^2}{\sigma^3}\sum_i E|X_i^3|+\frac{\sqrt{28}M^{3/2}}{\sqrt{\pi}\sigma^2}\sqrt{\sum_i E|X_i^4|}\]</span>
where <span class="math inline">\(d_W\)</span> is the Wasserstein distance.</p>
</div>
<div id="scaling" class="section level3">
<h3>Scaling</h3>
<p>The example of the parametric inter-rater experiment suggests that <span class="math inline">\(M/n\to 0\)</span> should be enough for a central limit theorem, but the actual theorem seems to require a stronger condition. If the <span class="math inline">\(X_i\)</span> were identically distributed and <span class="math inline">\(\sigma^2\)</span> scaled as <span class="math inline">\(n\)</span>, the first term would have order <span class="math inline">\(O(M^2n/n^{3/2})\)</span> and we would need <span class="math inline">\(M^2n^{-1/2}\to 0\)</span>. However, in the setup of the inter-rater experiment, if the rater and object variance components are non-zero, we actually have <span class="math inline">\(\sigma^2\)</span> scaling as <span class="math inline">\(nM\)</span>, so <span class="math inline">\(\sigma^3\)</span> scales as <span class="math inline">\(M^{3/2}n^{3/2}\)</span>. The first term in the bound is <span class="math inline">\(O(M^{1/2}n^{-1/2})\)</span> and <span class="math inline">\(M/n\to 0\)</span> is indeed sufficient.</p>
<p>Janson’s result using cumulants instead of Stein’s method doesn’t need the extra variance components to be non-zero, but it does impose stronger conditions on the tails of the <span class="math inline">\(X_i\)</span>.</p>
<p>When working on asymptotic distributions for mixed-model parameters I’m reasonably happy to assume that true variance components are positive – if instead they are on the boundary of the parameter space, special arguments will be needed in any case. The variance assumptions are more of a concern when the graph-structured dependence is induced by sampling rather than by a model for the outcome.</p>
</div>

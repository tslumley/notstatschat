---
title: "When the bootstrap doesn’t work"
author: "Thomas Lumley"
date:  2017-02-01 
output: html_document
---



<p>The bootstrap always works, except sometimes. </p>
<p>By ‘works’ here, I mean in the weakest senses that the large-sample bootstrap variance correctly estimates the variance of the statistic, or that the large-scale percentile bootstrap intervals have their nominal coverage.  I don’t mean the stronger sense that someone like Peter Hall might use, that the bootstrap gives higher-order accurate confidence intervals.  So the bootstrap ‘works’ for the median, even though not as well as for smooth functions of the mean.</p>
<p>Here are the reasons I know of why the bootstrap might fail</p>
<p>0. <strong>Correlation</strong>. The one that everyone knows about nowadays.  If your data have structure, such as a time series, a spatial map, a carefully-structured experimental design, a multistage survey, a network, then you can’t hope to get the right distribution by resampling in a way that doesn’t respect that structure. </p>
<p>1. <strong>Constraints</strong>: Suppose <span class="math inline">\(X_n\sim N(\theta,1)\)</span> and we know <span class="math inline">\(\theta\geq 0\)</span>. The maximum likelihood estimator of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat\theta=\max(\bar X,0)\)</span>. If <span class="math inline">\(\theta&gt;0\)</span> there isn’t a problem asymptotically (or at a more sophisticated analysis, if <span class="math inline">\(\theta\gg 1/\sqrt{n}\)</span> there isn’t).  But if <span class="math inline">\(\theta=0\)</span> the sampling distribution of <span class="math inline">\(\hat\theta\)</span> is a 50:50 mixture of a spike at zero and the positive half of a <span class="math inline">\(N(0,n^{-1})\)</span> distribution.  The bootstrap distribution is also a mixture of a spike at zero and and a half-normal, but the mass on the spike does not converge to 0.5 (or to anything else) as the sample size increases. The problem is that the height of the spike is <span class="math inline">\(\Phi(\bar X\sqrt{n})\)</span>, so the height converges in distribution to <span class="math inline">\(U(0,1)\)</span>. </p>
<p>2. <strong>Extrema</strong>.  Consider <span class="math inline">\(X\sim U(\theta,1)\)</span>. The bootstrap replicates <span class="math inline">\(\theta^*\)</span> have a distribution that puts mass <span class="math inline">\(0.632=1-e^{-1}\)</span> on the smallest observation, <span class="math inline">\(e^{-1}(1-e^{-1})\approx 0.233\)</span> on the second smallest, and so on geometrically. We always have <span class="math inline">\(\theta^*\geq\hat\theta\)</span>, and the bootstrap distribution stays very discrete as the sample size increases.</p>
<p>3. <strong>Lack of smoothness (cube-root asymptotics)</strong> Tukey’s shorth, the mean of the shortest half of the data, converges to the mean at <span class="math inline">\(n^{-1/3}\)</span> rate instead of the usual <span class="math inline">\(n^{-1/2}\)</span>. The same is true for the least-median-of-squares regression line, the isotonic regression estimator, and the semiparametric maximum likelihood estimator of a convex density. These all have non-Normal limiting distributions and the bootstrap fails for all of them.</p>
<p>4. <strong>Lack of smoothness (worse)</strong> Suppose your statistic of interest is the proportion of the distribution represented by non-zero probability mass points. The proportion of non-unique observations estimates this, and is zero for a continuous distribution.  It’s not even close to zero for bootstraps of a sample from a continuous distribution.</p>
<p>5. <strong>Serious outliers</strong>: Suppose <span class="math inline">\(X\)</span> comes from a Cauchy distribution and you don’t realise and still try bootstrap inference for the mean. There isn’t a mean. The bootstrap intervals don’t even correctly cover the center of symmetry (where the mean would be if there was one). The problem here is that a large collection of new samples from the true distribution would contain outliers of very different sizes (some worse than the original sample).  The bootstrap replicates contain multiple outliers the same size as in the original sample.</p>
<p>6. <strong>Zero derivative</strong>. Suppose <span class="math inline">\(X\sim N(\sqrt{\theta},1)\)</span> with <span class="math inline">\(\theta\geq 0\)</span> and your statistic is <span class="math inline">\(\bar X^2.\)</span> [edit so that <span class="math inline">\(\theta\)</span> is what <span class="math inline">\(\bar X^2\)</span> estimates] If <span class="math inline">\(\theta&gt; 0\)</span>, that’s fine: <span class="math inline">\(\sqrt{n}(\hat\theta-\theta)\)</span> is asymptotically Normal and the bootstrap works. But if <span class="math inline">\(\theta=0\)</span>, <span class="math inline">\(\sqrt{n}(\hat\theta-\theta)\)</span> converges to point mass at zero, and it’s <span class="math inline">\(n(\hat\theta-\theta)\)</span> that converges to <span class="math inline">\(\chi^2_1\)</span>.  Since <span class="math inline">\(\theta^*\)</span> is not taken from a distribution with mean zero (it has mean <span class="math inline">\(\bar X\)</span>), the convergence doesn’t work. The distributions are not regular at <span class="math inline">\(\theta=0\)</span>. For a couple of realistic examples, essentially this happens to the IDI statistic in medical diagnostics, and also in regression if you try to test the joint null hypothesis <span class="math inline">\(\theta_1=0 \cup \theta_2=0\)</span> using <span class="math inline">\(\hat\theta_1\hat\theta_2\)</span> as the statistic. [edit for ‘pleiotropy’ in genetics]</p>
<p>7. <strong>Sparse estimators</strong>: The lasso, for example, doesn’t bootstrap.  The problem is related to numbers 1 and 6: zero is a special value for the regression coefficients, and the distribution of estimates changes as the true regression coefficient approaches zero.</p>
<p>8. <strong>Overfitting</strong>:  If your predictive model is overfitted, all the bootstrap replicates will also be overfitted.  You need some way to subtract off the optimism. </p>
<p>Many of these are fixable with variations on the bootstrap: there are time-series bootstraps and survey bootstraps for problem 0, ideas like the .632 bootstrap for problem 8 and subtle penalised and thresholded bootstraps for problem 7. There are also subsampling bootstraps, which work (in theory) very generally as long as you know the convergence rate of your estimator.  But the simple nonparametric bootstrap can fail, and it’s good to have some idea of when it does. </p>

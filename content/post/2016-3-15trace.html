---
title: "Trace estimators and impact factors"
author: "Thomas Lumley"
date:  2016-03-15
output: html_document
---



<p>For a Secret Project™, I needed a quick estimator of the trace of a matrix. To be precise, I have a rectangular matrix <span class="math inline">\(A\)</span> and I needed <span class="math inline">\(\mathop{tr}(B)\)</span> and <span class="math inline">\(\mathop{tr}(B^2)\)</span> where <span class="math inline">\(B=A^TA\)</span>. That sounds easy, but <span class="math inline">\(A\)</span> is big enough that you don’t want to compute <span class="math inline">\(A^TA\)</span>. </p>
<p>The first one actually is easy: <span class="math display">\[\mathop{tr}(B)=\sum_{ij}(A_{ij})^2.\]</span> The second one is harder.  I tried a sampling approach: estimating a sample of the entries of <span class="math inline">\(B\)</span> and using <span class="math display">\[\mathop{tr}(B^2)=\sum_{ij} (B_{ij})^2.\]</span>  The performance was not great, even when I tried stratified sampling – getting all the diagonal entries of <span class="math inline">\(B\)</span> and a random sample of the off-diagonal entries.</p>
<p>I should have Googled before thinking, rather than after thinking. There is a standard solution: Hutchinson’s randomised trace estimator. If <span class="math inline">\(z\)</span> is a vector whose entries are independent, mean zero, unit variance, then <span class="math display">\[E[z^TBz]=\mathop{tr}(B).\]</span> <br />
So, if you have <span class="math inline">\(k\)</span> random vectors <span class="math inline">\(z_1,\dots,z_k\)</span>, a trace estimator is <br />
<span class="math display">\[\widehat{\mathop{tr}}(B^2)=\frac{1}{k} \sum_i z_i^TB^2z_i=\frac{1}{k} \sum_i z_iA^TAA^TAz_i=\frac{1}{k} \sum_i \left\|A^TAz_i\right\|_2^2\]</span></p>
<p>Hutchinson used Rademacher (random <span class="math inline">\(\pm 1\)</span>) variables for the entries of <span class="math inline">\(z\)</span>. You could also use standard Normals and it wouldn’t make much difference – it would affect what theoretical tools you could use to get tail bounds.  There’s a slight benefit in having the <span class="math inline">\(z_i\)</span> be uncorrelated, so in one application where it was convenient I used vectors <span class="math inline">\(Az\)</span> produced by the subsampled randomised Hadamard transform, but when <span class="math inline">\(A\)</span> is sparse and that isn’t convenient I just used standard Normals. You don’t get high accuracy – the central limit theorem says the error is of order <span class="math inline">\(k^{-1/2}\)</span> – but you get a reasonable estimator quickly.</p>
<p>The Hutchinson trace estimator has also been influential in thinking about degrees of freedom for semiparametric smoothers, as Grace Wahba mentions in her <a href="http://arxiv.org/abs/1303.5153">chapter</a> of “Past, Present, Future of Statistical Science”.</p>
<p>Since I knew <span class="math inline">\(\mathop{tr}(B)\)</span> I could improve on the Hutchinson estimator by ratio estimation from survey sampling. If you estimate <span class="math inline">\(\mathop{tr}(B)\)</span> using the same <span class="math inline">\(z\)</span>, a better estimator of <span class="math inline">\(\mathop{tr}(B^2)\)</span> is<br />
<span class="math display">\[\widehat{\mathop{tr}}_{\mathrm{ratio}}(B^2)= \widehat{\mathop{tr}}(B^2) \frac{\mathop{tr}(B)}{ \hat{\mathop{tr}}(B)}\]</span><br />
The sampling errors in the two estimators are correlated, so the error in the ratio is smaller – by about 50% in my case. </p>
<p>So, impact factors? Hutchinson’s <a href="http://www.tandfonline.com/doi/abs/10.1080/03610919008812866">paper</a> “A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines” is in the journal ‘Communications in Statistics - Simulation and Computation’, well known for not being especially picky or well read. It still has quite a few gems, such as Hutchinson’s paper and Pepe &amp; Anderson’s <a href="http://www.tandfonline.com/doi/abs/10.1080/03610919408813210#.Vuj8wBiriko">paper</a> about conditioning on <span class="math inline">\(X\)</span> at all times or just the current time in longitudinal data analysis.  “This paper is crap because it’s in a crap journal” has the same form as the classical <em>argumentum ad hominem</em>. It saves effort, but it’s still a fallacy.</p>

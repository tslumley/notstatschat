---
title: Estimator vs estimate
author: Thomas Lumley
date: '2024-06-27'
slug: estimator-vs-estimate
categories: []
tags: []
---



<p>In sandwich variance estimators, the middle of the sandwich<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> is the variance of the estimating function. If we have independent observations and estimate by solving
<span class="math display">\[\sum_i U_i(\theta)=0\]</span>
we want <span class="math inline">\(\mathrm{var}[\sum_i U_i(\theta)]\)</span>, which we estimate by <span class="math inline">\(\sum_i U_i(\hat\theta)U_i(\hat\theta)^T\)</span>. There are two issues here you might miss.</p>
<p>First, some people (younger me, for example) worry that <span class="math inline">\(\sum U_i(\hat\theta)\)</span> is always zero, by construction, so that surely <span class="math inline">\(\mathrm{var}\left[\sum U_i(\hat\theta)\right]\)</span> will also be zero. This is a nice simple mistake of confusing the random variable <span class="math inline">\(\hat\theta\)</span> with its value in a particular data set. In our data, we might have <span class="math inline">\(\hat\theta=42.69\)</span>, so that
<span class="math display">\[\sum_i U_i(42.69)=0.\]</span>
In other data sets sampled from the same distribution, <span class="math inline">\(\hat\theta\)</span> will have some other value and <span class="math inline">\(\sum_i U_i(42.69)\)</span> should be small, but it won’t be zero. When we write
<span class="math display">\[\mathrm{var}\left[\sum_i U_i(\hat\theta)\right]\]</span>
the notation <span class="math inline">\(\hat\theta\)</span> doesn’t mean the estimator as a random variable, it means the value that the estimator took in this sample. We want
<span class="math display">\[\mathrm{var}_{\theta=42.69}\left[\sum_i U_i(42.69)\right].\]</span></p>
<p>In other data sets sampled from the same distribution we won’t have <span class="math inline">\(\hat\theta=42.69\)</span>, so <span class="math inline">\(\mathrm{var}\left[\sum U_i(42.69)\right]\)</span> is not zero and is a reasonable estimator of <span class="math inline">\(\mathrm{var}\left[\sum U_i(\theta_0)\right]\)</span> if 42.69 is close to <span class="math inline">\(\theta_0\)</span>, which we know happens with high probability.</p>
<p>And, in addition, (assuming independence)
<span class="math display">\[\mathrm{var}_{\theta=42.69}\left[\sum_i U_i(42.69)\right].\]</span>
is well estimated by
<span class="math display">\[\sum_{i} U_i(42.69)U_i(42.69)^T.\]</span>
which we can evaluate without assuming a particular parametric model to compute the variances. We can prove this without a great deal of subtlety: just use Chebyshev’s inequality on the iid sum.</p>
<p>Things get more interesting when we have dependence. We need to include some crossproduct terms <span class="math inline">\(U_iU_j^T\)</span> in the variance calculation, representing non-zero covariances of <span class="math inline">\(U_i\)</span> and <span class="math inline">\(U_j\)</span>. It’s still true<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> that
<span class="math display">\[E\left[\sum_{i,j} U_i(\theta_0)U_j(\theta_0)\right]=\mathrm{var}\left[\sum_i U_i(\theta_0)\right],\]</span>
but it’s no longer true that</p>
<p><span class="math display">\[\sum_{i,j} U_i(\hat\theta)U_j(\hat\theta)\approx\mathrm{var}\left[\sum_i U_i(\theta_0)\right].\]</span>
The left-hand side of this is <strong>identically zero</strong>, just like it wasn’t for the iid case. The left-hand side evaluates to
<span class="math display">\[ \left(\sum_i U_i(\hat\theta)\right)\left(\sum_i U_i(\hat\theta)\right)^T=00^T.\]</span>
That’s actually a bit surprising. The bias<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> <a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> from using <span class="math inline">\(\hat\theta\)</span> instead of <span class="math inline">\(\theta_0\)</span> has grown from asymptotically negligible to the whole estimator. <a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>The difference between <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\hat\theta\)</span> is <span class="math inline">\(O_p(n^{-1/2})\)</span>, so the difference between <span class="math inline">\(U_i^2(\theta_0)\)</span> and <span class="math inline">\(U_i^2(\hat\theta)\)</span> is <span class="math inline">\(O_p(n^{-1})\)</span>, giving a bias of size <span class="math inline">\(O_p(1)\)</span> in a variance matrix that’s of order <span class="math inline">\(n\)</span>. So in the iid case the bias is asymptotically negligible. In the completely correlated case we have <span class="math inline">\(n^2\)</span> copies of the bias, so it’s only <span class="math inline">\(O_p(n)\)</span>. That doesn’t <em>prove</em> there’s a problem, since <span class="math inline">\(O_p\)</span> terms only give upper bounds, but we already knew there was a problem and it makes sense.</p>
<p>This counting argument suggests that we need <span class="math inline">\(o(n^2)\)</span> terms in the sum to get a sandwich estimator that works. Or, given that we’d like a little margin to get a rate of convergence, maybe <span class="math inline">\(O(n^{2-\delta})\)</span> terms. That’s potentially a problem – <span class="math inline">\(n\times n\)</span> is not <span class="math inline">\(O(n^{2-\delta})\)</span><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> – but in some settings we <strong>know</strong> that some of the <span class="math inline">\(\mathrm{cov}[U_i,U_j]\)</span> terms are exactly zero and we can leave them out.</p>
<p>In longitudinal data with <span class="math inline">\(m\)</span> observations on each of <span class="math inline">\(M\)</span> units, we have <span class="math inline">\(n=mM\)</span> data points but all pairs <span class="math inline">\((i,j)\)</span> of observations on two distinct units will have zero covariance. We can replace <span class="math inline">\(U_i(\hat\theta)U_j(\hat\theta)\)</span> for those pairs by just 0. The number of pairs we have left is <span class="math inline">\(Mm^2\)</span>, so if <span class="math inline">\(M\to\infty\)</span> we have <span class="math inline">\(Mm^2/n^2\to 0\)</span> and if <span class="math inline">\(M\to\infty\)</span> and <span class="math inline">\(m\)</span> is bounded by <em>any</em> power of <span class="math inline">\(M\)</span> we have <span class="math inline">\(Mm^2=O(n^{2-\delta})\)</span>. In <a href="https://notstatschat.rbind.io/2021/09/18/crossed-clustering-and-parallel-invention/">crossed clustering</a> with <span class="math inline">\(m\)</span> and <span class="math inline">\(M\)</span> distinct groups of two types, again we need <span class="math inline">\(m,M\to\infty\)</span> and <span class="math inline">\(m\)</span> bounded above and below by some power of <span class="math inline">\(M\)</span>. In both these cases the proofs are again just brute-forcing counting applications of Chebyshev’s inequality and a suitable Taylor series expansion</p>
<p>For time series and spatial data it’s a bit more tricky since we don’t ever have exact independence of <span class="math inline">\(U_i\)</span> and <span class="math inline">\(U_j\)</span>. Here we need to drop <span class="math inline">\((i,j)\)</span> terms where <span class="math inline">\(\mathrm{cov}[U_i,U_j]\)</span> is small enough, and we need to chose the threshold for <em>small enough</em> to get stricter with increasing <span class="math inline">\(n\)</span> at a rate that lets us use Chebyshev’s inequality on the non-zero terms we keep but still bound the bias from dropping terms.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> <a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p>So, in conclusion, sometimes seems like it should matter that we’re using <span class="math inline">\(\hat\theta\)</span> instead of <span class="math inline">\(\theta_0\)</span> in the sandwich estimator but it really doesn’t, and sometimes it doesn’t seem like it should matter but it really does.</p>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p><em>meat</em> for people who take their metaphors too seriously, <em>cheese</em> for people who take their metaphors too seriously and are vegetarian<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>yes, under some assumptions. I’m happy to assume as many finite moments as I need<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>which I called <em>centering</em> bias in my PhD thesis<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>well, “centring bias”, because Americans<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>you might ask: what if we used <span class="math inline">\(\theta_0\)</span> in the estimator instead of <span class="math inline">\(\hat\theta\)</span>? It still doesn’t work: <span class="math inline">\(\left(\sum_i U_i(\theta_0)\right)^{\otimes 2}\)</span> is unbiased for the variance but it’s not consistent<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>except perhaps to ChatGPT<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>I called it <em>truncation bias</em> in my thesis, which Americans don’t spell weirdly<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>If you want to make assumptions on the outcome variable that imply correlation thresholds on the estimating functions, you need to look up <em>mixing coefficients</em><a href="#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

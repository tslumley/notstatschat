---
title: A plug-in uniform law of large numbers
author: Thomas Lumley
date: '2022-09-28'
slug: uniform-law-of-large-numbers
categories: []
tags: []
---



<p><em>Attention Conservation Notice: I’m writing this down because I just spent too long trying to find a citation for it to give a student. A useful citation for many purposes is Newey WK (1989) “Uniform Convergence in Probability and Stochastic Equicontinuity” <a href="https://www.princeton.edu/~erp/ERParchives/archivepdfs/M342.pdf">Princeton Econometric Research Memorandum No 342</a></em></p>
<p>There are lots of laws of large numbers: theorems whose conclusions are that an average <span class="math inline">\(\bar X_n\)</span> converges in some useful sense to an expected value <span class="math inline">\(\mu\)</span>. You might have the simplest weak LLN: if <span class="math inline">\(X_i\)</span> are iid and <span class="math inline">\(E[X_i^2]\)</span> and <span class="math inline">\(\mu=E[X_i]\)</span> exist then
<span class="math display">\[\bar X_n-\mu\stackrel{p}{\to}0.\]</span>
You might have a law of large numbers for independent but not identically distributed sequences. Or for strong mixing random fields, or survey samples, or sparsely correlated sequences, or sequences of random functions, or whatever. Many of these are fairly easy to prove<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>; some are much more difficult<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. For notational reasons I’ll assume we have a weak law of large numbers, with convergence in probability, but the argument goes through the same for other modes of convergence.</p>
<p>The next complication is that you might have more than one such sequence. If you have finitely many sequences (<span class="math inline">\(K\)</span>, say), and <span class="math inline">\(\bar X_{nk}\)</span> is the mean of the first <span class="math inline">\(n\)</span> elements of sequence <span class="math inline">\(k\)</span>, then
<span class="math display">\[\sup_{k\in 1\dots K} \left| \bar X_{nk}-\mu_k\right|\stackrel{p}{\to}0\]</span>
because the supremum is just a finite maximum.</p>
<p>If you have infinitely many sequences, things can go wrong. There is, however, a fairly simple intermediate setting, where <span class="math inline">\(X(\theta)\)</span> is indexed by a parameter <span class="math inline">\(\theta\)</span> taking values in a compact set <span class="math inline">\(\Theta\)</span>. The problem with general <span class="math inline">\(\Theta\)</span> is that weird stuff can happen at the remote edges, and the point of a compact set is that it doesn’t have any remote edges for weird stuff to happen at.</p>
We’ll obviously need a bit more in the way of assumptions, because assuming <span class="math inline">\(\Theta\)</span> is compact gets you precisely nothing if <span class="math inline">\(X(\theta)\)</span> doesn’t have some well-behavedness as a function of <span class="math inline">\(\theta\)</span>. I will assume <span class="math inline">\(X(\theta)\)</span> is Lipschitz in <span class="math inline">\(\theta\)</span>, in the sense that there is a metric <span class="math inline">\(d()\)</span> on <span class="math inline">\(\Theta\)</span> and a random variable <span class="math inline">\(M\)</span> with finite mean such that for any <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>,
<span class="math display">\[\left|X_i(\theta_1)-X_i(\theta_2)\right|\leq Md(\theta_1,\theta_2)\]</span>
<hr>
<p>Theorem: If <span class="math inline">\(\bar X_n(\theta)\stackrel{p}{\to}\mu(\theta)\)</span> pointwise and <span class="math inline">\(X_i(\theta)\)</span> satisfies the stochastic Lipschitz condition for all <span class="math inline">\(\theta\)</span> in a compact set <span class="math inline">\(\Theta\)</span>, then
<span class="math display">\[\sup_{\theta\in\Theta}\left |\bar X_n(\theta)-\mu(\theta)\right|\stackrel{p}{\to}0\]</span></p>
<p>Proof:
Let <span class="math inline">\(\epsilon\)</span> be given and choose <span class="math inline">\(\theta_1\dots\theta_K\)</span> so that the balls of radius <span class="math inline">\(\epsilon\)</span> around the <span class="math inline">\(\theta_k\)</span> cover all of <span class="math inline">\(\Theta\)</span> (using compactness). For any <span class="math inline">\(\theta\in\Theta\)</span> and the <span class="math inline">\(\theta_k\)</span> it belongs to,
<span class="math display">\[ |\bar X_n(\theta)-\mu(\theta)|&lt; |X_n(\theta)-X_n(\theta_k)|+|\bar X_n(\theta_k)-\mu(\theta_k)|+|\mu(\theta_k)-\mu(\theta)|\]</span>
The middle term goes to zero in probability as <span class="math inline">\(n\to\infty\)</span> by the pointwise law of large numbers, the first and last terms are bounded in probability by <span class="math inline">\(M\epsilon\)</span> and bounded by <span class="math inline">\(E[M]\epsilon\)</span> respectively.</p>
<hr>
<p>This proof is reminiscent of the classical proof of the Glivenko-Cantelli Lemma. It’s a bit different, because it uses a Lipschitz condition rather than monotonicity to bound the first and last terms. Notably, the Glivenko-Cantelli Lemma does not need the index set <span class="math inline">\(\Theta\)</span> to be compact and does not need <span class="math inline">\(X_i(\theta)\)</span> and <span class="math inline">\(\mu(\theta)\)</span> to even be continuous.</p>
<p>My result above is not quite the theorem given in Newey’s report; he does not assume that the function <span class="math inline">\(\bar X_n(\theta)\)</span> is actually a sample average. That’s important because assuming it’s a sample average is valuable: for the analogue of the Glivenko-Cantelli theorem where you just have an arbitrary sequence <span class="math inline">\(F_n\)</span> of monotone functions converging to a monotone limit <span class="math inline">\(F\)</span>, the convergence is in general <em>not</em> uniform on intervals containing a discontinuity of <span class="math inline">\(F\)</span>. The key thing about a sample average is that the points stay where you put them: they accumulate, but they don’t move around. The counterexamples to uniform convergence for monotone functions involve the discontinuities moving – eg <span class="math inline">\(F_n\)</span> has a jump at <span class="math inline">\(1-\frac{1}{n}\)</span> and <span class="math inline">\(F\)</span> has a jump at 1. Newey needs some additional assumptions to get equicontinuity as the price for generalising beyond sample averages.</p>
<p>For comparison, here’s the Glivenko-Cantelli lemma, upgraded to take a plug-in pointwise LLN.</p>
<hr>
<p>Theorem: Let <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i=1\dots n\)</span> be a sequence of real-value random variables with common marginal distribution <span class="math inline">\(F\)</span>. Let
<span class="math display">\[\mathbb{F}_n(t)=\frac{1}{n}\sum_{i=1}^n I(X\leq t)\]</span>
be the empirical cumulative distribution function. If <span class="math inline">\(\mathbb{F}_n(t)\stackrel{p}{\to}F(t)\)</span> for every fixed <span class="math inline">\(t\)</span> then
<span class="math display">\[\sup_t\left|\mathbb{F}_n(t)-F(t)\right|\stackrel{p}{\to}0\]</span>
Proof: Let <span class="math inline">\(\epsilon&gt;0\)</span> be given. Choose <span class="math inline">\(-\infty=t_1&lt; t_2&lt;\cdots&lt; t_K=\infty\)</span> so that <span class="math inline">\(\max_k (F(t_K)-F(t_{k-1}))&lt;\epsilon\)</span>, where <span class="math inline">\(F(-\infty)=0\)</span> and <span class="math inline">\(F(\infty)=1\)</span>. For any <span class="math inline">\(t\)</span>, let <span class="math inline">\(t^*\)</span> be the first <span class="math inline">\(t_k\)</span> with <span class="math inline">\(t\leq t_k\)</span>
<span class="math display">\[\mathbb{F}_n(t)-F(t)\leq (F(t^*)-F(t))+|\mathbb{F}_n(t^*)-F(t^*)|\leq \epsilon+\max_k|\mathbb{F}_n(t_k)-F(t_k)|\]</span>
By assumption, we have pointwise (and thus uniform) convergence over <span class="math inline">\(k\)</span>, so the second term on the right goes to zero in probability.</p>
<p>Similarly,
<span class="math display">\[ \mathbb{F}_n(t)-F(t)\geq (F(t^*)-F(t))-|\mathbb{F}_n(t^*)-F(t^*)|\geq\epsilon-\max_k|\mathbb{F}_n(t^*)-F(t^*)|\]</span>
which again goes to <span class="math inline">\(\epsilon\)</span> in probability.</p>
Since <span class="math inline">\(\epsilon\)</span> was arbitrary we are done.
<hr>
<p>The two proofs can be reunited by using bracketing numbers, which gives the proof I actually want to present. I got this proof from <em>Weak Convergence and Empirical Processes</em> by van der Vaart and Wellner, where it’s about a third of the first page of Chapter 2.4, but (a) not everyone has read that book,<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> and (b) the proof is presented there for iid sequences, so it’s not that useful to me as a citation.</p>
<p>To start with, we need to define bracketing numbers. Given two functions <span class="math inline">\(\ell\)</span> and <span class="math inline">\(u\)</span>, the bracket <span class="math inline">\([\ell,u]\)</span> is the set of functions <span class="math inline">\(f\)</span> with <span class="math inline">\(\ell\leq f\leq u\)</span>. An <span class="math inline">\(\epsilon\)</span>-bracket in some norm <span class="math inline">\(\|\cdot\|\)</span>is a bracket with <span class="math inline">\(\|u-l\|\leq \epsilon.\)</span> The bracketing number <span class="math inline">\(N_{[\,]}(\epsilon,\|\cdot\|,{\cal F})\)</span> of a set <span class="math inline">\({\cal F}\)</span> of functions is the minimum number of <span class="math inline">\(\epsilon\)</span>-brackets needed to cover <span class="math inline">\({\cal F}\)</span>.</p>
<hr>
<p>Theorem: Let <span class="math inline">\({\cal F}\)</span> be a class of measurable functions and <span class="math inline">\(X_i\)</span> a sequence such that (i) <span class="math inline">\(f(X_i)\)</span> satisfies a weak law of large numbers for each <span class="math inline">\(f\in{\cal F}\)</span> and (ii) the bracketing numbers <span class="math inline">\(N_{[\,]}(\epsilon,\|\cdot\|,{\cal F})\)</span> are finite for every <span class="math inline">\(\epsilon&gt;0\)</span>. The law of large numbers holds uniformly over <span class="math inline">\(f\in{\cal F}\)</span>.</p>
<p>Proof:
Let <span class="math inline">\(\epsilon&gt;0\)</span> be given. Write <span class="math inline">\(\bar X_n(f)\)</span> for <span class="math inline">\(\frac{1}{n}\sum_i f(X_i)\)</span> and <span class="math inline">\(\mu(f)\)</span> for <span class="math inline">\(E[f(X)]\)</span>. Choose <span class="math inline">\(N_{[\,]}(\epsilon,\|\cdot\|,{\cal F})\)</span> <span class="math inline">\(\epsilon\)</span>-brackets that cover <span class="math inline">\({\cal F}\)</span>. Write <span class="math inline">\(u_f\)</span> for the upper function in the bracketing containing <span class="math inline">\(f\)</span> and <span class="math inline">\(\ell_f\)</span> for the corresponding lower function.</p>
<p><span class="math display">\[\bar X_n(f)-\mu(f)\leq(\mu(u_f)-\mu(f))+|\bar X_n(u_f)-\mu(u_f)|\]</span>
so
<span class="math display">\[\begin{align}\sup_{f\in{\cal F}} (\bar X_n(f)-\mu(f)) &amp;\leq \sup_{f\in{\cal F}}(\mu(u_f)-\mu(f))+ \sup_{f\in{\cal F}}|\bar X_n(u_f)-\mu(u_f)|\\&amp;=\sup_{f\in{\cal F}}(\mu(u_f)-\mu(f))+ \max_{i}|\bar X_n(u_i)-\mu(u_I)|\end{align}\]</span>
The first term is bounded by <span class="math inline">\(\epsilon\)</span> (by construction). Because there are only finitely many distinct <span class="math inline">\(u_i\)</span> and we assume pointwise convergence, the second term converges to zero in probability as <span class="math inline">\(n\to\infty\)</span>.</p>
<p>Similarly
<span class="math display">\[\bar X_n(f)-\mu(f)\geq(\mu(u_f)-\mu(f))-|\bar X_n(u_f)-\mu(u_f)|\]</span>
so
<span class="math display">\[\begin{align}\inf_{f\in{\cal F}} (\bar X_n(f)-\mu(f)) &amp;\geq \inf_{f\in{\cal F}}(\mu(u_f)-\mu(f))- \inf_{f\in{\cal F}}|\bar X_n(u_f)-\mu(u_f)|\\&amp;=\inf_{f\in{\cal F}}(\mu(u_f)-\mu(f))- \min_{i}|\bar X_n(u_i)-\mu(u_I)|\end{align}\]</span>
The first term is bounded by <span class="math inline">\(\epsilon\)</span> (by construction); the second term again converges to zero in probability as <span class="math inline">\(n\to\infty\)</span>.</p>
<p>Since <span class="math inline">\(\epsilon\)</span> was arbitrary, we are done.</p>
<hr>
<p>The classical Glivenko-Cantelli lemma is the case where <span class="math inline">\({\cal F}\)</span> is the set of indicators of half-lines: <span class="math inline">\({\cal F}=\{f_t(x)=I(x\geq t), t\in\mathbb{R}\}\)</span>, whose bracketing numbers that are <span class="math inline">\(O(\epsilon^{-1})\)</span>. The first result above is for a set of functions indexed by a compact metric space <span class="math inline">\(\Theta\)</span>, where the functions are Lipschitz in the parameter. The bracketing numbers are proportional to the number of <span class="math inline">\(\epsilon\)</span>-balls it takes to cover <span class="math inline">\(\Theta\)</span>, which is finite for every <span class="math inline">\(\epsilon\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>This Lipschitz-in-the-parameter example is also valuable because the bracketing numbers are small enough that you get a uniform central limit theorem for <span class="math inline">\({\cal F}\)</span> with independent data – though, sadly, there aren’t plug-in uniform CLTs for dependent data.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> The uniform CLT for iid data is Theorem 5.31 of van der Vaart’s <em>Asymptotic Statistics</em> (though it’s not actually proved until Chapter 19). For independent but not iid data I think you need the van der Vaart and Wellner book, Theorem 2.11.9 and its corollaries and follow-ups.</p>
<p>There are a few similar uniform CLT results for dependent data; for example, there’s a <a href="http://www.numdam.org/item/?id=AIHPB_1995__31_2_393_0%5D">very pretty argument</a> by Doukhan, Massart and Rio for stationary <span class="math inline">\(\beta\)</span>-mixing sequences. Most of the work is for stationary sequences, because probabilists understandably don’t want to complicate the interesting and intricate arguments needed to handle dependence with boring and intricate arguments needed to handle non-stationary sequences. One <a href="https://doi.org/10.2307/1403549">useful exception</a> is by Andrews and Pollard, handling not necessarily stationary strong mixing sequences.</p>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Chebyshev’s inequality is your friend<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Chebyshev’s inequality is probably still your friend, but the sort of friend that’s vaguely supportive without being helpful<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>shocking, I know<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>The property of having this sort of finite cover for every <span class="math inline">\(\epsilon\)</span> is called “total boundedness”. A compact set in a metric space is totally bounded, and a totally bounded set in a metric space has compact closure.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The hard part… one of the hard parts… is proving tightness, which involves expectations of suprema over <span class="math inline">\(\theta\)</span>, and these genuinely are affected by the dependence. There aren’t even fully standardised ways to do the proofs, though something like Bernstein’s inequality is a good start<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

---
title: Assumptions
author: Thomas Lumley
date: '2024-04-14'
slug: assumptions
categories: []
tags: []
---



<p>One problem in teaching statistics and communicating statistics and so on is “assumptions”.
In fact, there’s at least two problems:</p>
<div id="necessary-vs-sufficient" class="section level2">
<h2>Necessary vs sufficient</h2>
<p>The first problem is in maths communication. In maths you write down some assumptions and show they imply a conclusion. It’s usual in statistics for the assumptions to be sufficient for the conclusion, but pretty unusual for them to be necessary. People are bad at explaining the distinction to statisticians.</p>
<p>For example, if <span class="math inline">\(Y|X\stackrel{iid}{\sim} N(X\beta,\sigma^2)\)</span> the ordinary least squares estimator is unbiased and Normally distribution for <span class="math inline">\(\beta\)</span>. The proof that this is true assumes that the conditional distributions of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> are Normal. It’s an assumption of the <strong>theorem</strong>. It’s not, however, an assumption of the <strong>estimator</strong>.</p>
<p>If <span class="math inline">\((X,Y)\)</span> is a set of independent draws from some joint distribution with enough finite moments, the ordinary least squares estimator is consistent (and asymptotically unbiased) for the best-fitted straight line to the joint distribution, as is asymptotically Normal. That, too, is a theorem, with different assumptions.</p>
<p>As intermediate cases, you might have <span class="math inline">\(E[Y\mid X=x]=x\beta\)</span> but have equal-variance errors that aren’t necessarily Normal, or mean zero errors that aren’t necessarily equal-variance or other things. Or you might not have independence between observations but have some sort of approximate independence. These also lead to theorems, with different assumptions</p>
<p>These regressions are not all equal. The parametric linear model lets you get confidence intervals for <span class="math inline">\(E[Y|X=x]\)</span> and prediction intervals for <span class="math inline">\(Y\)</span>, and lets you prove <span class="math inline">\(\hat\beta\)</span> is the best estimator of the true <span class="math inline">\(\beta\)</span>. The completely non-parametric version doesn’t let you get confidence intervals for <span class="math inline">\(E[Y]\)</span> or prediction intervals for <span class="math inline">\(Y\)</span> and it doesn’t even make sense to ask if <span class="math inline">\(\hat\beta\)</span> is optimal because there isn’t any true <span class="math inline">\(\beta\)</span>. In general, the more you assume, the more conclusions you can draw.</p>
<p>So, one reason for unhelpful discussion about assumptions is that people don’t distinguish necessary from sufficient conditions, and don’t think about which parts of the properties of an estimator they care about. People talk about “the assumptions” of least-squares regression as if that meant something.</p>
<p>A particular problem with the “necessary vs sufficient” issue is that when you’re teaching <strong>proofs</strong> it makes sense to lead with the simplest proofs, which often have the strongest assumptions. When you’re not teaching proofs, though, it makes no sense to impose unnecessary assumptions that would make proofs easier for hypothetical people who aren’t your students.</p>
</div>
<div id="assumptions-vs-choices" class="section level2">
<h2>Assumptions vs choices</h2>
<p>Consider the assumption that the errors have mean zero in a linear regression model. What is the <em>statistical</em> content of this assumption? How could it be wrong? What would that mean about data definition and collection?</p>
<p>Most of the time, there’s no assumption of mean-zero errors; it’s just a choice. The mean of the residuals and the intercept of the fitted linear relationship are not separately identifiable. We <strong>choose</strong> to say the mean of the errors is zero, so that the line goes through the mean of the the conditional distributions and the intercept is <span class="math inline">\(E[Y|X=0]\)</span></p>
<p>Occasionally there might be implications. The lung function measure <span class="math inline">\(FEV_1\)</span> is the amount of air you can blow into a tube in 1 second. Because you can use a spirometer incorrectly, the true value is <a href="https://www.ncbi.nlm.nih.gov/books/NBK540970/"><em>defined</em></a> as the maximum value an individual can generate. If that’s the definition, the <em>measurement</em> error in <span class="math inline">\(FEV_1\)</span> does not have mean zero; it’s necessarily positive It still could be that the error <span class="math inline">\(Y-X\beta\)</span> has mean zero, but now there might be an assumption lurking there.</p>
<p>Choices also figure in regression functional form: if you want to estimate the effect of smoking on heart attack incidence then you will have a smoking variable <span class="math inline">\(X\)</span> and a set of confounders <span class="math inline">\(Z\)</span> and it’s a genuine assumption that <span class="math inline">\(Z\)</span> is correctly modelled and sufficient to block all the confounding paths from <span class="math inline">\(X\)</span> to heart attack. It’s a <em>choice</em> whether you model smoking as binary yes/no, or three-level current/former/never, or as a detailed lifetime history of tobacco use. Comparing smokers to non-smokers doesn’t <em>assume</em> that all smokers are the same (though if they aren’t, the precise answer you get will, of course, depend on which smokers you have).</p>
</div>
